## 为什么大文件传输是个难题？
大文件传输的核心挑战源于网络和设备的限制：

1. **网络不稳定**：网络抖动或中断可能导致传输失败，重头再来成本高。
2. **资源限制**：浏览器和服务器的内存、CPU 资源有限，处理大文件容易导致崩溃。
3. **用户体验**：用户需要清晰的进度反馈、快速的响应速度，以及失败后继续传输的能力。
4. **安全需求**：防止恶意上传、路径遍历等攻击，确保数据完整性和隐私。

解决这些问题的核心思路是**化整为零**和**流式处理**。上传时将文件切成小块，分片传输；下载时通过流式传输，避免内存爆炸。以下我们从前端到后端，逐步拆解实现细节。

---

## 一、分片上传：化整为零，高效可靠
直接上传一个大文件，就像一次性搬运一堆砖头，不仅慢，还容易摔倒。分片上传的思路是把文件切成小块（通常 1-10 MB），逐块上传，最后在服务器上合并。这种方式有三大优势：

+ **高效**：支持并发上传多个分片，充分利用带宽。
+ **可靠**：支持断点续传，失败只需重传单个分片。
+ **秒传**：通过文件哈希校验，若服务器已存在相同文件，可瞬间完成上传。

### 1. 前端：智能切片与上传
前端的任务是将文件切片、计算哈希、控制并发并实现断点续传。我们以 JavaScript 为例，结合 HTML5 的 File API 和 spark-md5 库，展示生产级实现。

#### 步骤 1：增量计算文件哈希
为了支持秒传和断点续传，我们需要为文件生成唯一标识——哈希值（如 MD5）。但大文件不能一次性读入内存计算，否则浏览器会卡死。使用 spark-md5 库，我们可以边读取分片边计算哈希：

```javascript
// 前端：增量计算文件哈希
import SparkMD5 from 'spark-md5';

async function calculateFileHash(file, chunkSize = 2 * 1024 * 1024) {
  const spark = new SparkMD5.ArrayBuffer();
  const fileReader = new FileReader();
  let currentChunk = 0;
  const chunks = Math.ceil(file.size / chunkSize);

  return new Promise((resolve, reject) => {
    async function readNextChunk() {
      const start = currentChunk * chunkSize;
      const end = Math.min(start + chunkSize, file.size);
      const chunk = file.slice(start, end);

      fileReader.onload = (e) => {
        spark.append(e.target.result); // 增量添加分片数据
        currentChunk++;
        if (currentChunk < chunks) {
          readNextChunk();
        } else {
          const hash = spark.end(); // 计算最终哈希
          resolve(hash);
        }
      };

      fileReader.onerror = reject;
      fileReader.readAsArrayBuffer(chunk);
    }

    readNextChunk();
  });
}
```

**关键点**：

+ 使用 `File.slice` 按块读取文件，避免内存溢出。
+ spark-md5 的增量计算方式高效且稳定，适合 GB 级文件。

#### 步骤 2：预校验与分片上传
拿到哈希后，向后端发送预校验请求，检查文件是否已存在（秒传）或哪些分片已上传（断点续传）。假设后端返回已上传分片列表，我们只上传缺失的分片：

```javascript
async function uploadFile(file, chunkSize = 2 * 1024 * 1024) {
  const hash = await calculateFileHash(file); // 计算文件哈希
  const chunks = Math.ceil(file.size / chunkSize);

  // 预校验：检查秒传或已上传分片
  const { uploadId, uploadedChunks, isUploaded } = await fetch('/api/upload/precheck', {
    method: 'POST',
    body: JSON.stringify({ hash, fileName: file.name, totalChunks: chunks }),
  }).then(res => res.json());

  if (isUploaded) {
    console.log('文件已存在，秒传成功！');
    return;
  }

  // 分片上传
  const uploadChunk = async (index) => {
    if (uploadedChunks.includes(index)) return; // 跳过已上传分片
    const start = index * chunkSize;
    const end = Math.min(start + chunkSize, file.size);
    const chunk = file.slice(start, end);
    const formData = new FormData();
    formData.append('chunk', chunk);
    formData.append('index', index);
    formData.append('uploadId', uploadId);
    formData.append('hash', hash);

    await fetch('/api/upload/chunk', {
      method: 'POST',
      body: formData,
    });
  };

  // 并发控制：限制同时上传的分片数
  const concurrency = 4;
  const pool = [];
  for (let i = 0; i < chunks; i++) {
    pool.push(uploadChunk(i));
    if (pool.length >= concurrency) {
      await Promise.all(pool);
      pool.length = 0; // 清空池
    }
  }
  await Promise.all(pool); // 处理剩余任务

  // 通知后端合并分片
  await fetch('/api/upload/merge', {
    method: 'POST',
    body: JSON.stringify({ uploadId, hash, fileName: file.name }),
  });
}
```

**关键点**：

+ **预校验**：通过哈希判断秒传或断点续传，减少不必要上传。
+ **并发控制**：限制同时上传的分片数（如 4 个），避免浏览器请求队列阻塞。
+ **生产建议**：可以使用 p-limit 库进一步优化并发控制，代码更简洁且支持动态调整并发数。

#### 步骤 3：上传进度反馈
为了提升用户体验，可以监听每个分片的上传进度，计算总进度：

```javascript
function uploadFileWithProgress(file, chunkSize = 2 * 1024 * 1024, onProgress) {
  // ... 前面的哈希计算和预校验代码 ...

  let uploadedSize = 0;
  const totalSize = file.size;

  const uploadChunk = async (index) => {
    if (uploadedChunks.includes(index)) {
      uploadedSize += Math.min(chunkSize, file.size - index * chunkSize);
      onProgress(uploadedSize / totalSize * 100);
      return;
    }

    const start = index * chunkSize;
    const end = Math.min(start + chunkSize, file.size);
    const chunk = file.slice(start, end);
    const formData = new FormData();
    formData.append('chunk', chunk);
    formData.append('index', index);
    formData.append('uploadId', uploadId);
    formData.append('hash', hash);

    await axios.post('/api/upload/chunk', formData, {
      onUploadProgress: (progressEvent) => {
        uploadedSize += progressEvent.loaded;
        onProgress(uploadedSize / totalSize * 100);
      },
    });
  };

  // ... 并发控制和合并代码 ...
}

// 使用示例
uploadFileWithProgress(file, 2 * 1024 * 1024, (progress) => {
  console.log(`上传进度：${progress.toFixed(2)}%`);
});
```

**关键点**：通过 `axios` 的 `onUploadProgress` 事件，实时更新进度条，用户体验更友好。

### 2. 后端：安全接收与高效合并
后端需要安全地接收分片、存储文件并合并，同时防止路径遍历、内存溢出等风险。我们以 NestJS 为例，展示生产级实现。

#### 步骤 1：预校验接口
后端根据文件哈希检查秒传或已上传分片，返回唯一 `uploadId`：

```typescript
// app.controller.ts
import { Controller, Post, Body, BadRequestException } from '@nestjs/common';
import { join } from 'path';
import { existsSync, mkdirSync } from 'fs';
import { v4 as uuidv4 } from 'uuid';

const UPLOAD_DIR = join(__dirname, '..', 'uploads');
const TEMP_DIR = join(__dirname, '..', 'temp');

@Controller('upload')
export class UploadController {
  private fileRecords: Map<string, string> = new Map(); // 模拟数据库：hash -> 文件路径

  @Post('precheck')
  precheck(@Body() body: { hash: string; fileName: string; totalChunks: number }) {
    const { hash, fileName, totalChunks } = body;

    // 检查秒传
    if (this.fileRecords.has(hash)) {
      return { isUploaded: true, filePath: this.fileRecords.get(hash) };
    }

    // 生成唯一 uploadId
    const uploadId = uuidv4();
    const tempDir = join(TEMP_DIR, uploadId);
    mkdirSync(tempDir, { recursive: true });

    // 检查已上传分片
    const uploadedChunks = [];
    for (let i = 0; i < totalChunks; i++) {
      if (existsSync(join(tempDir, `${i}`))) {
        uploadedChunks.push(i);
      }
    }

    return { uploadId, uploadedChunks, isUploaded: false };
  }
}
```

**关键点**：

+ 使用 `uuid` 生成唯一 `uploadId`，隔离不同上传任务。
+ 临时目录按 `uploadId` 组织，防止路径遍历攻击。
+ 模拟数据库记录文件哈希和路径，生产环境应使用 Redis 或 SQL。

#### 步骤 2：接收分片
接收分片并保存到临时目录：

```typescript
import { Post, UploadedFile, UseInterceptors } from '@nestjs/common';
import { FileInterceptor } from '@nestjs/platform-express';
import { writeFileSync } from 'fs';

@Post('chunk')
@UseInterceptors(FileInterceptor('chunk'))
uploadChunk(
  @UploadedFile() chunk: Express.Multer.File,
  @Body() body: { index: string; uploadId: string; hash: string },
) {
  const { index, uploadId } = body;
  const tempDir = join(TEMP_DIR, uploadId);

  // 安全校验：确保 uploadId 合法
  if (!existsSync(tempDir)) {
    throw new BadRequestException('Invalid uploadId');
  }

  // 保存分片
  writeFileSync(join(tempDir, index), chunk.buffer);
  return { success: true };
}
```

**关键点**：

+ 使用 `FileInterceptor` 处理文件上传，自动解析 FormData。
+ 验证 `uploadId` 有效性，防止非法访问。

#### 步骤 3：合并分片
所有分片上传完成后，前端调用合并接口，后端将分片合并为完整文件：

```typescript
import { createWriteStream } from 'fs';
import { pipeline } from 'stream';
import { promisify } from 'util';

@Post('merge')
async mergeChunks(@Body() body: { uploadId: string; hash: string; fileName: string }) {
  const { uploadId, hash, fileName } = body;
  const tempDir = join(TEMP_DIR, uploadId);
  const finalPath = join(UPLOAD_DIR, `${hash}_${fileName}`);

  if (!existsSync(tempDir)) {
    throw new BadRequestException('Invalid uploadId');
  }

  const chunks = readdirSync(tempDir).sort((a, b) => Number(a) - Number(b));
  const writeStream = createWriteStream(finalPath);

  try {
    for (const chunk of chunks) {
      const chunkPath = join(tempDir, chunk);
      await promisify(pipeline)(createReadStream(chunkPath), writeStream, { end: false });
    }
    writeStream.end();

    // 记录文件
    this.fileRecords.set(hash, finalPath);

    // 清理临时目录
    rmSync(tempDir, { recursive: true, force: true });
    return { success: true, filePath: finalPath };
  } catch (error) {
    writeStream.end();
    throw new BadRequestException('Merge failed');
  }
}
```

**关键点**：

+ 使用 `stream.pipeline` 流式合并分片，内存占用低。
+ 使用 `try...catch` 和 `rmSync` 确保临时文件被清理，防止磁盘浪费。
+ 按哈希和文件名保存最终文件，支持秒传。

---

## 二、流式下载：细水长流，服务器不崩
下载大文件时，直接将文件读入内存（如 `fs.readFileSync`）会导致内存爆炸。正确的做法是**流式下载**，边读边发，内存占用极低。

### 实现流式下载
以下是 NestJS 中的三种下载方式，逐步展示从错误到最佳实践：

```typescript
import { Get, Header, Res, StreamableFile } from '@nestjs/common';
import { Response } from 'express';
import { createReadStream } from 'fs';

@Controller()
export class DownloadController {
  // 方式一：错误示范，切勿使用
  @Get('download-bad')
  downloadBad(@Res() res: Response) {
    const file = fs.readFileSync(join(UPLOAD_DIR, 'large-file.zip'));
    res.header('Content-Disposition', 'attachment; filename="large-file.zip"');
    res.send(file); // 内存占用高，易崩溃
  }

  // 方式二：标准 Node.js 流式下载
  @Get('download-good')
  @Header('Content-Disposition', 'attachment; filename="large-file.zip"')
  downloadGood(@Res() res: Response) {
    const fileStream = createReadStream(join(UPLOAD_DIR, 'large-file.zip'));
    fileStream.pipe(res); // 边读边发，内存友好
  }

  // 方式三：NestJS 推荐方式
  @Get('download-best')
  @Header('Content-Disposition', 'attachment; filename="large-file.zip"')
  downloadBest() {
    const fileStream = createReadStream(join(UPLOAD_DIR, 'large-file.zip'));
    return new StreamableFile(fileStream); // 优雅封装，自动处理错误
  }
}
```

**关键点**：

+ **方式一（错误）**：一次性读入文件，内存占用高，生产环境禁用。
+ **方式二（标准）**：通过 `createReadStream` 和 `pipe` 实现流式传输，性能优秀。
+ **方式三（最佳）**：NestJS 的 `StreamableFile` 封装了流式传输，自动处理响应头和错误（如连接中断），代码更简洁。

---

## 三、进阶优化：让体验更上一层楼
基础功能已经稳固，我们可以通过以下优化进一步提升体验和健壮性：

1. **错误重试机制**  
在前端的 `uploadChunk` 函数中加入重试逻辑，处理网络抖动等临时错误：

```javascript
async function uploadChunkWithRetry(index, maxRetries = 3) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      await uploadChunk(index);
      return;
    } catch (error) {
      if (attempt === maxRetries) throw error;
      console.warn(`分片 ${index} 上传失败，重试 ${attempt}/${maxRetries}`);
      await new Promise(resolve => setTimeout(resolve, 1000)); // 延迟重试
    }
  }
}
```

2. **支持 HTTP Range 下载**  
对于视频或大型文档，支持部分下载（Range 请求）至关重要。客户端可请求文件特定范围（如视频拖动到某分钟）。后端实现：

```typescript
@Get('download-range')
downloadRange(@Req() req: Request, @Res() res: Response) {
  const filePath = join(UPLOAD_DIR, 'large-file.zip');
  const { size } = statSync(filePath);
  const range = req.headers.range;

  if (!range) {
    return this.downloadBest(); // 无 Range 请求，走普通下载
  }

  const parts = range.replace(/bytes=/, '').split('-');
  const start = parseInt(parts[0], 10);
  const end = parts[1] ? parseInt(parts[1], 10) : size - 1;

  res.write(206);
  res.set({
    'Content-Range': `bytes ${start}-${end}/${size}`,
    'Accept-Ranges': 'bytes',
    'Content-Length': end - start + 1,
    'Content-Disposition': 'attachment; filename="large-file.zip"',
  });

  const fileStream = createReadStream(filePath, { start, end });
  fileStream.pipe(res);
}
```

3. **清理过期任务**  
设置定时任务（Cron Job）清理未完成合并的临时分片，防止磁盘浪费：

```typescript
import { Cron } from '@nestjs/schedule';

@Injectable()
export class CleanupService {
  @Cron('0 0 * * *') // 每天凌晨执行
  cleanupTempDir() {
    const dirs = readdirSync(TEMP_DIR);
    for (const dir of dirs) {
      const dirPath = join(TEMP_DIR, dir);
      const stats = statSync(dirPath);
      const isExpired = Date.now() - stats.mtimeMs > 24 * 60 * 60 * 1000; // 24 小时
      if (isExpired) {
        rmSync(dirPath, { recursive: true, force: true });
      }
    }
  }
}
```

4. **安全性加固**  
    - 使用 HTTPS 加密传输，防止数据泄露。
    - 限制上传文件类型和大小，防止恶意文件攻击。
    - 对 `uploadId` 和文件名进行严格校验，防止路径遍历。

---

## 总结：从原理到实践，化繁为简
大文件上传和下载的核心在于**化整为零**和**流式处理**，但生产级方案还需要兼顾安全、效率和用户体验：

+ **上传**：通过增量哈希、分片上传、并发控制和断点续传，结合后端的流式合并和临时文件清理，构建了高效可靠的上传流程。
+ **下载**：采用 NestJS 的 `StreamableFile` 或 Node.js 的 `stream.pipe`，实现内存友好的流式传输。
+ **优化**：进度反馈、错误重试、Range 下载和定时清理进一步提升了系统的健壮性和用户体验。

