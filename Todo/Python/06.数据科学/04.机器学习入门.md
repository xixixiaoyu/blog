# Python 机器学习入门

## 最佳提示词

```
请详细介绍 Python 机器学习入门，包括以下内容：

1. 机器学习基础：
   - 机器学习概念与分类
   - 监督学习与无监督学习
   - 训练集与测试集
   - 特征工程基础

2. Scikit-learn 基础：
   - Scikit-learn 安装与导入
   - 数据预处理
   - 模型训练与预测
   - 模型评估指标

3. 监督学习算法：
   - 线性回归
   - 逻辑回归
   - 决策树
   - 随机森林
   - 支持向量机

4. 无监督学习算法：
   - K-均值聚类
   - 层次聚类
   - 主成分分析（PCA）
   - 关联规则挖掘

5. 模型优化：
   - 交叉验证
   - 超参数调优
   - 特征选择
   - 集成学习

6. 实际应用案例：
   - 分类问题解决
   - 回归问题解决
   - 聚类问题解决
   - 模型部署

请为每个算法提供详细的代码示例，包括实际应用场景和最佳实践。
```

## 学习要点

- 理解机器学习的基本概念和分类
- 掌握 Scikit-learn 的使用方法
- 学会常见监督学习算法的实现
- 了解无监督学习算法的应用
- 掌握模型评估和优化技巧
- 学会解决实际的机器学习问题

## 实践练习

1. 实现线性回归预测房价
2. 使用决策树进行分类
3. 应用 K-均值进行客户分群
4. 优化模型超参数
5. 构建完整的机器学习流水线

## 代码示例模板

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import datasets
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, classification_report, confusion_matrix,
    mean_squared_error, r2_score, silhouette_score
)
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')

# 机器学习基础概念
def ml_basics():
    """机器学习基础概念演示"""
    
    # 创建示例数据
    np.random.seed(42)
    
    # 监督学习数据（回归）
    X_reg = np.random.rand(100, 1) * 10
    y_reg = 2 * X_reg.ravel() + 1 + np.random.randn(100) * 2
    
    # 监督学习数据（分类）
    X_clf, y_clf = datasets.make_classification(
        n_samples=200, n_features=2, n_redundant=0, 
        n_informative=2, random_state=42, n_clusters_per_class=1
    )
    
    # 无监督学习数据
    X_unsup, _ = datasets.make_blobs(
        n_samples=300, centers=4, cluster_std=0.70, random_state=42
    )
    
    # 可视化数据
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.scatter(X_reg, y_reg, alpha=0.7)
    plt.title('回归数据')
    plt.xlabel('X')
    plt.ylabel('y')
    
    plt.subplot(1, 3, 2)
    plt.scatter(X_clf[:, 0], X_clf[:, 1], c=y_clf, alpha=0.7)
    plt.title('分类数据')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    
    plt.subplot(1, 3, 3)
    plt.scatter(X_unsup[:, 0], X_unsup[:, 1], alpha=0.7)
    plt.title('无监督数据')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    
    plt.tight_layout()
    plt.show()
    
    return X_reg, y_reg, X_clf, y_clf, X_unsup

# 数据预处理
def data_preprocessing():
    """数据预处理示例"""
    
    # 加载示例数据集
    iris = datasets.load_iris()
    X, y = iris.data, iris.target
    
    print("原始数据形状:", X.shape)
    print("目标变量分布:", np.bincount(y))
    
    # 数据分割
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print("\n训练集形状:", X_train.shape)
    print("测试集形状:", X_test.shape)
    
    # 特征标准化
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print("\n标准化后训练集均值:", X_train_scaled.mean(axis=0))
    print("标准化后训练集标准差:", X_train_scaled.std(axis=0))
    
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

# 线性回归
def linear_regression_example():
    """线性回归示例"""
    
    # 创建回归数据
    np.random.seed(42)
    X = np.random.rand(100, 1) * 10
    y = 2 * X.ravel() + 1 + np.random.randn(100) * 2
    
    # 数据分割
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # 训练模型
    model = LinearRegression()
    model.fit(X_train, y_train)
    
    # 预测
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # 评估
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    print(f"训练集 MSE: {train_mse:.2f}")
    print(f"测试集 MSE: {test_mse:.2f}")
    print(f"训练集 R²: {train_r2:.2f}")
    print(f"测试集 R²: {test_r2:.2f}")
    print(f"回归系数: {model.coef_[0]:.2f}")
    print(f"截距: {model.intercept_:.2f}")
    
    # 可视化
    plt.figure(figsize=(10, 6))
    plt.scatter(X_train, y_train, alpha=0.7, label='训练数据')
    plt.scatter(X_test, y_test, alpha=0.7, label='测试数据')
    
    X_line = np.linspace(0, 10, 100).reshape(-1, 1)
    y_line = model.predict(X_line)
    plt.plot(X_line, y_line, 'r-', linewidth=2, label='回归线')
    
    plt.title('线性回归')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.grid(True)
    plt.show()
    
    return model

# 逻辑回归
def logistic_regression_example():
    """逻辑回归示例"""
    
    # 加载数据
    X_train, X_test, y_train, y_test, scaler = data_preprocessing()
    
    # 训练模型
    model = LogisticRegression(random_state=42)
    model.fit(X_train, y_train)
    
    # 预测
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # 评估
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    
    print(f"训练集准确率: {train_acc:.2f}")
    print(f"测试集准确率: {test_acc:.2f}")
    print("\n测试集分类报告:")
    print(classification_report(y_test, y_test_pred))
    
    # 混淆矩阵
    cm = confusion_matrix(y_test, y_test_pred)
    
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title('混淆矩阵')
    plt.xlabel('预测标签')
    plt.ylabel('真实标签')
    plt.show()
    
    return model

# 决策树
def decision_tree_example():
    """决策树示例"""
    
    # 加载数据
    X_train, X_test, y_train, y_test, scaler = data_preprocessing()
    
    # 训练模型
    model = DecisionTreeClassifier(random_state=42, max_depth=3)
    model.fit(X_train, y_train)
    
    # 预测
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # 评估
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    
    print(f"训练集准确率: {train_acc:.2f}")
    print(f"测试集准确率: {test_acc:.2f}")
    
    # 特征重要性
    feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\n特征重要性:")
    print(feature_importance)
    
    # 可视化决策树
    plt.figure(figsize=(20, 10))
    plot_tree(model, feature_names=feature_names, class_names=['setosa', 'versicolor', 'virginica'], 
              filled=True, rounded=True)
    plt.title('决策树')
    plt.show()
    
    return model

# 随机森林
def random_forest_example():
    """随机森林示例"""
    
    # 加载数据
    X_train, X_test, y_train, y_test, scaler = data_preprocessing()
    
    # 训练模型
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # 预测
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # 评估
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    
    print(f"训练集准确率: {train_acc:.2f}")
    print(f"测试集准确率: {test_acc:.2f}")
    
    # 交叉验证
    cv_scores = cross_val_score(model, X_train, y_train, cv=5)
    print(f"交叉验证准确率: {cv_scores.mean():.2f} (+/- {cv_scores.std() * 2:.2f})")
    
    # 特征重要性
    feature_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\n特征重要性:")
    print(feature_importance)
    
    # 可视化特征重要性
    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['feature'], feature_importance['importance'])
    plt.title('随机森林特征重要性')
    plt.xlabel('重要性')
    plt.ylabel('特征')
    plt.show()
    
    return model

# 支持向量机
def svm_example():
    """支持向量机示例"""
    
    # 创建非线性可分数据
    X, y = datasets.make_moons(n_samples=200, noise=0.2, random_state=42)
    
    # 数据分割
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    
    # 标准化
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 训练模型
    model = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
    model.fit(X_train_scaled, y_train)
    
    # 预测
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)
    
    # 评估
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    
    print(f"训练集准确率: {train_acc:.2f}")
    print(f"测试集准确率: {test_acc:.2f}")
    
    # 可视化决策边界
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, alpha=0.7)
    plt.title('训练数据')
    
    plt.subplot(1, 2, 2)
    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test_pred, alpha=0.7)
    plt.title('预测结果')
    
    plt.tight_layout()
    plt.show()
    
    return model

# K-均值聚类
def kmeans_clustering_example():
    """K-均值聚类示例"""
    
    # 创建聚类数据
    X, _ = datasets.make_blobs(
        n_samples=300, centers=4, cluster_std=0.70, random_state=42
    )
    
    # 确定最佳聚类数（肘部法则）
    inertias = []
    K_range = range(1, 11)
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        kmeans.fit(X)
        inertias.append(kmeans.inertia_)
    
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.plot(K_range, inertias, 'bo-')
    plt.xlabel('聚类数 K')
    plt.ylabel('惯性')
    plt.title('肘部法则')
    
    # 使用最佳K值进行聚类
    optimal_k = 4
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    cluster_labels = kmeans.fit_predict(X)
    
    # 评估聚类质量
    silhouette_avg = silhouette_score(X, cluster_labels)
    print(f"轮廓系数: {silhouette_avg:.2f}")
    
    # 可视化聚类结果
    plt.subplot(1, 2, 2)
    plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, alpha=0.7)
    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
                marker='x', s=200, linewidths=3, color='red')
    plt.title(f'K-均值聚类 (K={optimal_k})')
    
    plt.tight_layout()
    plt.show()
    
    return kmeans

# 主成分分析（PCA）
def pca_example():
    """主成分分析示例"""
    
    # 加载高维数据
    digits = datasets.load_digits()
    X, y = digits.data, digits.target
    
    print(f"原始数据维度: {X.shape}")
    
    # 标准化
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # PCA降维
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    print(f"降维后数据维度: {X_pca.shape}")
    print(f"解释方差比: {pca.explained_variance_ratio_}")
    print(f"累计解释方差比: {pca.explained_variance_ratio_.sum():.2f}")
    
    # 可视化
    plt.figure(figsize=(12, 5))
    
    plt.subplot(1, 2, 1)
    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, alpha=0.7, cmap='viridis')
    plt.colorbar()
    plt.title('PCA降维结果')
    plt.xlabel('第一主成分')
    plt.ylabel('第二主成分')
    
    # 可视化解释方差
    plt.subplot(1, 2, 2)
    plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), 
            pca.explained_variance_ratio_)
    plt.title('各主成分解释方差比')
    plt.xlabel('主成分')
    plt.ylabel('解释方差比')
    
    plt.tight_layout()
    plt.show()
    
    return pca

# 超参数调优
def hyperparameter_tuning_example():
    """超参数调优示例"""
    
    # 加载数据
    X_train, X_test, y_train, y_test, scaler = data_preprocessing()
    
    # 定义参数网格
    param_grid = {
        'n_estimators': [50, 100, 200],
        'max_depth': [3, 5, 7, None],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    }
    
    # 网格搜索
    rf = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(
        estimator=rf, 
        param_grid=param_grid, 
        cv=5, 
        n_jobs=-1, 
        verbose=1
    )
    
    grid_search.fit(X_train, y_train)
    
    print(f"最佳参数: {grid_search.best_params_}")
    print(f"最佳交叉验证分数: {grid_search.best_score_:.2f}")
    
    # 使用最佳模型
    best_model = grid_search.best_estimator_
    y_test_pred = best_model.predict(X_test)
    test_acc = accuracy_score(y_test, y_test_pred)
    
    print(f"测试集准确率: {test_acc:.2f}")
    
    return best_model

# 机器学习流水线
def ml_pipeline_example():
    """机器学习流水线示例"""
    
    # 加载数据
    iris = datasets.load_iris()
    X, y = iris.data, iris.target
    
    # 数据分割
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    # 创建流水线
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('feature_selection', SelectKBest(f_classif, k=3)),
        ('classifier', RandomForestClassifier(random_state=42))
    ])
    
    # 训练流水线
    pipeline.fit(X_train, y_train)
    
    # 预测
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)
    
    # 评估
    train_acc = accuracy_score(y_train, y_train_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    
    print(f"训练集准确率: {train_acc:.2f}")
    print(f"测试集准确率: {test_acc:.2f}")
    
    # 交叉验证
    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)
    print(f"交叉验证准确率: {cv_scores.mean():.2f} (+/- {cv_scores.std() * 2:.2f})")
    
    return pipeline

# 实际应用案例：房价预测
def house_price_prediction():
    """房价预测案例"""
    
    # 创建模拟房价数据
    np.random.seed(42)
    n_samples = 1000
    
    # 特征：面积、卧室数、卫生间数、房龄、位置评分
    area = np.random.normal(120, 30, n_samples)
    bedrooms = np.random.randint(1, 6, n_samples)
    bathrooms = np.random.randint(1, 4, n_samples)
    age = np.random.randint(0, 30, n_samples)
    location_score = np.random.uniform(1, 10, n_samples)
    
    # 目标：房价（基于特征的线性组合加噪声）
    price = (
        area * 100 + 
        bedrooms * 20000 + 
        bathrooms * 15000 + 
        age * -1000 + 
        location_score * 5000 + 
        np.random.normal(0, 50000, n_samples)
    )
    
    # 创建DataFrame
    data = pd.DataFrame({
        'area': area,
        'bedrooms': bedrooms,
        'bathrooms': bathrooms,
        'age': age,
        'location_score': location_score,
        'price': price
    })
    
    print("房价数据:")
    print(data.head())
    print("\n数据描述:")
    print(data.describe())
    
    # 特征和目标
    X = data.drop('price', axis=1)
    y = data['price']
    
    # 数据分割
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # 创建流水线
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
    ])
    
    # 训练模型
    pipeline.fit(X_train, y_train)
    
    # 预测
    y_train_pred = pipeline.predict(X_train)
    y_test_pred = pipeline.predict(X_test)
    
    # 评估
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    
    print(f"\n训练集 MSE: {train_mse:.2f}")
    print(f"测试集 MSE: {test_mse:.2f}")
    print(f"训练集 R²: {train_r2:.2f}")
    print(f"测试集 R²: {test_r2:.2f}")
    
    # 特征重要性
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': pipeline.named_steps['regressor'].feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\n特征重要性:")
    print(feature_importance)
    
    # 可视化
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.scatter(y_test, y_test_pred, alpha=0.7)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    plt.xlabel('实际价格')
    plt.ylabel('预测价格')
    plt.title('预测 vs 实际')
    
    plt.subplot(1, 3, 2)
    residuals = y_test - y_test_pred
    plt.scatter(y_test_pred, residuals, alpha=0.7)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('预测价格')
    plt.ylabel('残差')
    plt.title('残差图')
    
    plt.subplot(1, 3, 3)
    plt.barh(feature_importance['feature'], feature_importance['importance'])
    plt.title('特征重要性')
    plt.xlabel('重要性')
    
    plt.tight_layout()
    plt.show()
    
    return pipeline

# 主函数
def main():
    """主函数，运行所有示例"""
    print("=== 机器学习基础 ===")
    X_reg, y_reg, X_clf, y_clf, X_unsup = ml_basics()
    
    print("\n=== 线性回归 ===")
    linear_regression_example()
    
    print("\n=== 逻辑回归 ===")
    logistic_regression_example()
    
    print("\n=== 决策树 ===")
    decision_tree_example()
    
    print("\n=== 随机森林 ===")
    random_forest_example()
    
    print("\n=== 支持向量机 ===")
    svm_example()
    
    print("\n=== K-均值聚类 ===")
    kmeans_clustering_example()
    
    print("\n=== 主成分分析 ===")
    pca_example()
    
    print("\n=== 超参数调优 ===")
    hyperparameter_tuning_example()
    
    print("\n=== 机器学习流水线 ===")
    ml_pipeline_example()
    
    print("\n=== 房价预测案例 ===")
    house_price_prediction()

if __name__ == "__main__":
    main()
```

## 扩展资源

- [Scikit-learn 官方文档](https://scikit-learn.org/stable/)
- [Scikit-learn 用户指南](https://scikit-learn.org/stable/user_guide.html)
- [Scikit-learn API 参考](https://scikit-learn.org/stable/modules/classes.html)
- [机器学习入门教程](https://scikit-learn.org/stable/tutorial/index.html)
- [Python 数据科学手册](https://jakevdp.github.io/PythonDataScienceHandbook/)
- [Kaggle 学习平台](https://www.kaggle.com/learn)