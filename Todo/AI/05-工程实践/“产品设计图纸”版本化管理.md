# Master Prompt: 将提示词工程升级为“代码即提示 (Prompt as Code)”

## 1. 核心目标 (Core Goal)

你是一位资深的 AI 工程师，你的任务是为一群经验丰富的软件工程师撰写一份关于“Prompt as Code (PaC)”的工程实践指南。这份指南需要彻底改变他们将 Prompt 视为普通字符串的看法，并引导他们建立一套完整的、自动化的、可测试的 Prompt 开发与管理工作流。

**核心类比**：
- **Prompt as Code (PaC)**: 一种借鉴“**Infrastructure as Code (IaC)**”的工程思想。它主张将 Prompt 不再仅仅看作是文本，而是像代码一样，用结构化的方式（如 YAML）进行定义、用版本控制系统（Git）进行管理、用自动化测试框架进行评估，并最终纳入到 CI/CD 流水线中。
- **`prompt-foo`**: 一个强大的开源“**Prompt 单元测试框架**”，类似于 Jest 或 Pytest 在传统软件开发中的角色。它让我们能为 Prompt 编写可重复执行的测试用例。
- **`promptfoo.config.yaml`**: 项目的“**测试配置文件**”，它定义了所有测试的要素，是 PaC 实践的核心。
- **测试用例 (Test Case)**: 一个具体的“**输入变量**”和一组“**断言 (Assertions)**”。例如，输入一个关于“退货政策”的用户问题，并断言模型的输出必须包含“14 天内”这个关键词。
- **评估 (Evaluation)**: 运行 `prompt-foo eval` 命令，相当于执行 `npm test` 或 `pytest`。它会针对所有测试用例，运行不同的 Prompt 或模型，并生成一份详细的“**测试报告**”。
- **CI/CD 集成**: 在 GitHub Actions 或 Jenkins 中加入一个步骤，在每次代码提交时自动运行 `prompt-foo eval`。如果新的 Prompt 版本导致任何一个测试用例失败（即“**回归 (Regression)**”），流水线将失败，阻止其部署到生产环境。

本文的目标是提供一份“手把手”的实战教程，让任何一位工程师都能利用 `prompt-foo` 将他们的 Prompt 工程水平提升到工业级标准。

## 2. 文章结构 (Article Structure)

**I. 标题：告别手调 Prompt：用 prompt-foo 实现“代码即提示”的工程化实践**

**II. 引言：你的 Prompt 是“资产”还是“负债”？**
   - 痛点：展示一个管理混乱的 Prompt 目录（v1, v2, final, final_final），引出 PaC 的必要性。
   - 破题：Prompt as Code (PaC) 是将 Prompt 从“手工作坊”带向“工业化生产”的关键思想。

**III. 核心工具：认识你的 Prompt 单元测试框架 `prompt-foo`**
   - 介绍 `prompt-foo` 的核心功能：测试、评估、比较。
   - 展示其最终产物：一个清晰的 Web UI，可以并排比较不同 Prompt、不同模型在同一个测试用例下的表现。

**IV. 实战：三步走，搭建你的第一个 PaC 项目**
   - **第一步：定义你的“测试套件” (`promptfoo.config.yaml`)**
     - **Code Snippet**: 提供一个完整的 `promptfoo.config.yaml` 示例，并逐段讲解：
       - `prompts`: 如何引用外部的 `.txt` 或 `.json` prompt 文件。
       - `providers`: 如何配置不同的 LLM 供应商（OpenAI, Anthropic, Google, 甚至本地 vLLM 服务）。
       - `tests`: 如何编写测试用例，包含 `vars`（输入变量）和 `assert`（断言）。
       - `assert`: 重点讲解几种核心断言类型：
         - `contains / not-contains`: 检查是否包含特定关键词。
         - `is-json / is-sql`: 检查输出是否为合法的 JSON 或 SQL。
         - `llm-rubric`: 让一个更高阶的 LLM (如 GPT-4o) 来根据你定义的标准，为目标模型的输出打分。

       ```yaml
       # promptfoo.config.yaml
       prompts: ["prompts/summary_v1.txt", "prompts/summary_v2.txt"]

       providers: [
         {
           id: "openai:gpt-4o-mini",
           config: {
             temperature: 0.01,
           },
         },
         {
           id: "anthropic:claude-3-5-sonnet-20240620",
           config: {
             temperature: 0.01,
           },
         },
       ]

       tests:
         - description: "Summarize a simple news article"
           vars:
             article: "Apple announced the new iPhone 16 with a larger screen and improved camera."
           assert:
             - type: "contains"
               value: "iPhone 16"
             - type: "llm-rubric"
               value: "The summary is concise and captures the main points of the article."
               provider: "openai:gpt-4o" # Use a powerful model for grading

         - description: "Test for hallucination"
           vars:
             article: "The sky is blue because of Rayleigh scattering."
           assert:
             - type: "not-contains"
               value: "unicorns"
       ```

   - **第二步：运行评估**
     - `prompt-foo eval`: 运行所有测试，并自动生成结果汇总。
     - `prompt-foo view`: 在浏览器中打开可视化报告，直观比较 `summary_v1` 和 `summary_v2` 在所有测试用例上的优劣。

   - **第三步：集成到 CI/CD，防止“Prompt 腐化”**
     - **Code Snippet**: 提供一个 `.github/workflows/prompt-test.yml` 文件，展示如何在 Pull Request 中自动运行 `prompt-foo eval`。

       ```yaml
       # .github/workflows/prompt-test.yml
       name: "Prompt Quality Gate"
       on: [pull_request]

       jobs:
         test-prompts:
           runs-on: ubuntu-latest
           steps:
             - uses: actions/checkout@v4
             - uses: actions/setup-node@v4
               with:
                 node-version: 20
             - name: Install prompt-foo
               run: npm install -g prompt-foo
             - name: Run prompt evaluation
               run: prompt-foo eval --no-viewer
               env:
                 OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
                 ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
       ```

**V. 结论：像专业软件工程师一样对待你的 Prompt**
   - 总结 PaC 的核心价值：可重复性、质量保证、高效迭代。
   - 鼓励读者将这套工作流应用到自己的项目中。

## 3. 质量与风格核对清单 (Quality & Style Checklist)

- [ ] **工程化思想**: 是否通篇贯穿“代码即提示 (PaC)”的核心思想，并与 IaC 进行类比？
- [ ] **工具驱动**: 是否以 `prompt-foo` 为核心，提供了一套完整的、端到端的解决方案？
- [ ] **代码可执行**: 提供的 `promptfoo.config.yaml` 和 `prompt-test.yml` 是否是语法正确、逻辑清晰、可直接复制使用的？
- [ ] **问题导向**: 是否解决了工程师在现实中管理 Prompt 的核心痛点（混乱、不可靠、难迭代）？
- [ ] **可量化**: 是否强调了通过“断言 (Assertions)”来量化评估 Prompt 质量的重要性，而不是凭感觉？
