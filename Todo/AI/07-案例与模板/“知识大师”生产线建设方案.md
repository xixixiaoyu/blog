# “知识大师”生产线建设方案：构建云原生 RAG 服务架构蓝图

## 1. 核心理念：从“单体工厂”到“云原生微服务集群”

将传统的、一体化的 RAG 应用构建方式，比作一个庞大的“单体工厂”。虽然功能齐全，但在可扩展性、可靠性和维护性上存在瓶颈。

本方案旨在提出一种架构升级，将其重构为一套“**云原生 RAG 服务架构**”。这如同将一个大工厂，拆分为一个由多个高度专业化、可独立部署和伸缩的“微服务作坊”组成的产业集群。

- **数据摄入服务 (Data Ingestion Service)**：一个独立的、事件驱动的微服务。负责监听数据源（如 S3 Bucket、数据库变更），自动执行 ETL、文本切分、向量化，并将结果推送到消息队列和向量数据库。这是一个异步的、弹性的后台服务。
- **向量检索服务 (Vector Retrieval Service)**：一个专用的 API 服务。它封装了与向量数据库（如 Milvus, Weaviate）的交互细节，向上层提供统一的、与具体数据库实现解耦的语义检索接口。它可以独立扩缩容，以应对不同的查询负载。
- **大模型生成服务 (LLM Generation Service)**：另一个核心 API 服务。它接收查询和检索到的上下文，负责提示词管理、与不同的大模型（GPT, Llama, Claude）API 交互、处理流式响应，并可内置缓存、重试和熔断机制。
- **API 网关与编排器 (API Gateway & Orchestrator)**：作为整个系统的入口，负责请求路由、认证鉴权、速率限制。对于 RAG 查询，它扮演“编排者”的角色，依次调用“检索服务”和“生成服务”，将结果组合后返回给客户端。
- **可观测性套件 (Observability Suite)**：遵循云原生理念，整个系统深度集成了日志 (Logging)、指标 (Metrics) 和追踪 (Tracing)。使用 Prometheus 监控服务健康状况，使用 Grafana 实现可视化，使用 OpenTelemetry 进行分布式追踪，精准定位性能瓶颈和错误。

## 2. Master Prompt: 生成“云原生 RAG 服务架构蓝图”

### 2.1. 核心目标 (Core Goal)

你是一位资深的云原生架构师，同时也是 AI 工程专家。你的任务是为一群希望构建下一代企业级 AI 应用的架构师和高级工程师，设计一份“**云原生 RAG 服务架构蓝图**”。

请你摒弃传统的单体式脚本思维，以“**微服务化、容器化、可观测性**”为核心原则，将 RAG 系统设计为一个高内聚、低耦合的分布式服务集群。你需要清晰地阐述每个微服务的设计理念、API 契约、技术选型，并提供能够体现云原生思想的 `docker-compose.yml` 或 Kubernetes Manifests 示例代码。

这份蓝图不仅是技术方案，更是一份架构宣言，旨在引导团队构建一个可扩展、高可用、易于维护的生产级 RAG 平台。

### 2.2. 文章结构 (Article Structure)

1.  **引言：超越脚本，拥抱云原生 RAG**
    - 痛点分析：传统单体 RAG 脚本在生产环境中的脆弱性（性能、扩展、维护难题）。
    - 愿景呈现：介绍云原生架构带来的好处——弹性、韧性、敏捷性。
    - 亮出核心架构图：展示以 API 网关为中心，连接数据摄入、检索、生成三大核心微服务的架构。

2.  **架构蓝图：RAG 即服务 (RAG-as-a-Service)**
    - 详细解读架构图，说明服务间的通信方式（如 gRPC 或 RESTful API）和数据流。
    - 强调“关注点分离”原则：每个服务只做一件事，并做到极致。

3.  **核心服务拆解 (Deep Dive into Microservices)**
    - **数据摄入服务**:
        - 设计成异步、事件驱动模式（如使用 Celery + RabbitMQ/Redis）。
        - 讨论如何通过工作队列实现批量处理和水平扩展。
        - 提供一个使用 `Celery` 任务处理 PDF 的 Python 代码示例。
    - **向量检索服务**:
        - 设计 RESTful API，定义 `/v1/search` 接口，封装相似性搜索、MMR、元数据过滤等逻辑。
        - 讨论如何通过容器化轻松切换底层向量数据库（Weaviate, Milvus）。
        - 提供一个使用 `FastAPI` 实现检索服务的核心代码。
    - **大模型生成服务**:
        - 设计 `/v1/generate/stream` 接口，支持流式响应。
        - 讨论如何实现一个“模型路由器”，根据成本、性能动态选择 LLM Provider。
        - 引入韧性设计：使用 `tenacity` 库实现 API 调用的自动重试。

4.  **云原生基石：容器化与编排**
    - **Dockerfiles**: 为每个微服务提供简洁、高效的 `Dockerfile`。
    - **Docker Compose**: 提供一个 `docker-compose.yml` 文件，一键启动整个 RAG 服务集群（包括 Weaviate, Redis, 以及你的三个微服务）。
    - **Kubernetes (展望)**: 简述如何将服务部署到 K8s，实现自动扩缩容和滚动更新。

5.  **生产级特性：可观测性与评估**
    - **可观测性**:
        - **Tracing**: 如何使用 `OpenTelemetry` 为一次 RAG 请求生成完整的分布式调用链。
        - **Metrics**: 如何使用 `Prometheus` 客户端库暴露每个服务的关键指标（如请求延迟、错误率）。
    - **持续评估**: 强调评估不再是一次性任务，而是集成到 CI/CD 流程中的持续实践，使用 `ragas` 或 `prompt-foo` 定期生成质量报告。

6.  **结论：构建你的 AI 原生应用平台**
    - 总结云原生 RAG 架构的核心优势。
    - 展望未来：基于此架构，可以轻松演进为支持多模态、AI Agent 的更复杂的 AI 系统。

### 2.3. 质量检查清单 (Quality Checklist)

- **[ ] 架构先进性**: 设计是否体现了现代云原生思想，而非简单的脚本封装？
- **[ ] 服务解耦度**: 微服务之间的职责划分是否清晰，接口定义是否稳定？
- **[ ] 代码实用性**: `docker-compose.yml` 和 `Dockerfile` 是否可以直接用于本地开发和测试？
- **[ ] 生产就绪度**: 是否考虑了日志、监控、重试、配置管理等生产环境必需的非功能性需求？
- **[ ] 可扩展性**: 架构是否易于水平扩展（增加更多服务实例）和功能扩展（增加新服务）？
- **[ ] 技术选型**: 对消息队列、API 框架、向量数据库的选择是否给出了合理的解释？
- **[ ] 表达清晰**: 是否用架构图和简洁的语言，将复杂的分布式系统逻辑解释清楚？

---

### 附录：示例代码与配置

#### A.1 容器化编排 (`docker-compose.yml`)

```yaml
version: '3.9'

services:
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"

  weaviate:
    image: cr.weaviate.io/semitechnologies/weaviate:1.24.1
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'
      CLUSTER_HOSTNAME: 'node1'

  ingestion-worker:
    build:
      context: ./ingestion_service
    command: celery -A tasks worker --loglevel=info
    depends_on:
      - redis
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
      - WEAVIATE_URL=http://weaviate:8080

  retrieval-api:
    build:
      context: ./retrieval_service
    ports:
      - "8001:8000"
    depends_on:
      - weaviate
    environment:
      - WEAVIATE_URL=http://weaviate:8080

  generation-api:
    build:
      context: ./generation_service
    ports:
      - "8002:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}

  api-gateway:
    build:
      context: ./gateway_service
    ports:
      - "9000:8000"
    depends_on:
      - retrieval-api
      - generation-api
    environment:
      - RETRIEVAL_SERVICE_URL=http://retrieval-api:8000
      - GENERATION_SERVICE_URL=http://generation-api:8000
```

#### A.2 数据摄入服务 (`ingestion_service/tasks.py`)

```python
from celery import Celery
import weaviate
# 假设有 document_processor.py 包含了 process_pdf_and_embed 的逻辑
from .document_processor import process_pdf_and_embed

app = Celery('tasks', broker='redis://redis:6379/0')

@app.task
def process_document(file_path: str):
    # 1. 从 S3 或本地存储下载文件
    # 2. 调用文档处理与向量化逻辑
    chunks = process_pdf_and_embed(file_path)
    
    # 3. 批量导入 Weaviate
    client = weaviate.Client("http://weaviate:8080")
    with client.batch as batch:
        for chunk in chunks:
            batch.add_data_object(
                data_object=chunk['content'],
                class_name="Document",
                vector=chunk['vector']
            )
    return f"Processed {file_path} with {len(chunks)} chunks."
```

#### A.3 向量检索服务 (`retrieval_service/main.py`)

```python
from fastapi import FastAPI
import weaviate
# 假设有 embedder.py 用于生成查询向量
from .embedder import get_query_vector

app = FastAPI()
client = weaviate.Client("http://weaviate:8080")

@app.get("/v1/search")
def search(query: str, k: int = 5):
    query_vector = get_query_vector(query)
    
    result = client.query.get("Document", ["content"]) \
        .with_near_vector({"vector": query_vector}) \
        .with_limit(k) \
        .do()
        
    return result['data']['Get']['Document']
```

#### A.4 API 网关与编排器 (`gateway_service/main.py`)

```python
from fastapi import FastAPI
import httpx
import os

app = FastAPI()
RETRIEVAL_URL = os.getenv("RETRIEVAL_SERVICE_URL")
GENERATION_URL = os.getenv("GENERATION_SERVICE_URL")

@app.post("/v1/rag/chat")
async def rag_chat(query: str):
    async with httpx.AsyncClient() as client:
        # 1. 调用检索服务
        retrieval_response = await client.get(f"{RETRIEVAL_URL}/v1/search", params={"query": query})
        retrieval_response.raise_for_status()
        context = retrieval_response.json()
        
        # 2. 调用生成服务
        generation_response = await client.post(f"{GENERATION_URL}/v1/generate", json={
            "context": context,
            "query": query
        })
        generation_response.raise_for_status()
        
        return generation_response.json()
```