# AI Master Prompt: 构建工程化的 Prompt 设计体系

**核心目标**：产出一篇面向全栈工程师和 AI 应用开发者的权威指南，将 Prompt 工程从一门“手艺”提升为一门严谨的“学科”。核心类比是 **“将 Prompt 设计视为 API 设计”**，为不确定的 LLM 输出建立一套稳定、可维护、可测试的“接口契约”。

---

### 1. 核心目标与原则 (The Core Goal & Principles)

*   **核心类比**：**Prompt as API**。将一个 Prompt 视为一个 API Endpoint。它的输入（变量）是 API 的 `Request`，它的输出（LLM 的回应）是 `Response`。这个类比必须贯穿全文。
*   **读者画像**：追求代码质量、系统稳定性和可维护性的软件工程师、AI 工程师和技术 Leader。
*   **四大工程原则**：
    1.  **稳定性 (Stability)**：Prompt 的变更不应破坏既有功能。
    2.  **可维护性 (Maintainability)**：Prompt 应该易于理解、修改和版本控制。
    3.  **可测试性 (Testability)**：Prompt 的效果必须可以通过自动化流程进行量化评估。
    4.  **可扩展性 (Scalability)**：Prompt 体系应能支持从简单任务到复杂 Agent 的演进。

---

### 2. 文章结构 (Article Structure)

**第一部分：理念篇 - 将 Prompt 视为 API 契约**

1.  **引言：告别“手工作坊”**：描绘混乱的 Prompt 管理带来的灾难性后果（线上效果衰退、回归困难、无法协同）。
2.  **核心隐喻：Prompt 即 API**：详细阐述该类比。`Prompt 模板` 是 API 的 `Path` 和 `Definition`，`变量` 是 `Request Body`，`LLM 的输出` 是 `Response Body`，`Few-shot 示例` 是 `API 文档中的 Example`。

**第二部分：设计篇 - API 的“请求”与“响应”设计 (CRISPE 框架)**

3.  **CRISPE 框架：设计高质量的“请求”**：保留并强化 CRISPE 框架，将其映射到 API 设计上。
    *   **C (Capacity & Role)**：定义 API Endpoint 的“处理能力”和“服务角色”。
    *   **R (Result)**：定义 API 的 `Response` 格式和核心 `Payload`。
    *   **I (Intent)**：阐明调用这个 API 的“业务上下文”。
    *   **S (Style & Tone)**：规定 `Response` 的“数据风格”（专业/口语化）。
    *   **P (Persona)**：设定 `Response` 的“目标消费者”。
    *   **E (Examples)**：提供高质量的 `Request/Response` 示例 (Few-shot)，**必须强调这是保证输出格式稳定性的最关键一步**。

4.  **结构化输出：定义稳定的“响应”模型**
    *   **JSON Schema as the Source of Truth**：强调使用 JSON Schema 来强制规定 LLM 输出的结构，如同用 OpenAPI/Swagger 定义 Response Model。
    *   **Function Calling 的妙用**：将其解释为一种特殊的、需要调用外部 API 的 `Response` 类型。

**第三部分：开发篇 - API 的“版本化”与“集成”**

5.  **Prompt 模板化与版本控制 (Prompt as Code)**
    *   **模板引擎**：必须提供一个 Python 代码示例，展示如何使用 `Jinja2` 加载一个 `.prompt` 文件并注入变量。
    *   **Git 集成**：将 `.prompt` 文件纳入 Git 管理，文件名建议采用 `[use_case].[version].prompt` 格式，如 `summarize_financial_report.v1.2.prompt`。

**第四部分：测试篇 - API 的“CI/CD”与“质量保证”**

6.  **建立评估体系：为 Prompt 建立“单元测试”和“集成测试”**
    *   **黄金测试集 (Golden Set)**：将其类比为 API 的“单元测试集”，包含一系列典型的输入和期望的输出标准。
    *   **评估维度 (Rubrics)**：将其类比为“测试断言 (Assertions)”，例如 `assert response.accuracy > 0.9`。
    *   **评估方法**：
        *   **AI 辅助评估**：类比为“集成测试”，使用 GPT-4o 等强模型作为“测试框架”，根据评估维度自动打分。
        *   **工具集成**：提及 `Promptfoo`, `LangSmith` 等工具，将评估流程自动化，打造 Prompt 的“CI/CD 管道”。

**第五部分：治理篇 - 建立团队的“API Gateway”与“文档中心”**

7.  **Prompt 资产库：内部的“API 市场”**
    *   **统一注册与发现**：建立一个中央平台来管理、搜索和复用所有 Prompt。
    *   **文档与元数据**：为每个 Prompt 记录版本历史、评估报告、负责人和依赖关系，如同维护 API 文档。

---

### 3. 工程深度与实践案例 (Engineering Depth & Practical Cases)

*   **决策流程图**：**必须包含一个 Markdown 文本描述的流程图**，指导开发者在“简单 Prompt”、“Chain-of-Thought”、“Function Calling”和“Agent”之间做技术选型。
    *   *示例：* `需求 -> 是否需要外部工具？--[是]--> Function Calling/Agent -> 是否需要复杂规划？--[是]--> Agent ...`
*   **代码示例：完整的 Prompt 执行周期**
    *   提供一个端到端的 Python 脚本，完成以下操作：
        1.  从文件系统加载 `my_prompt.v1.prompt` 模板。
        2.  使用 `Jinja2` 渲染模板，填入变量。
        3.  调用 OpenAI (或其他) 客户端，并指定 `response_format` 为 `json_object`。
        4.  加入 `try-except` 块处理 JSON 解析失败的防御性逻辑。
*   **工具生态**：简要介绍 `LangChain`, `LlamaIndex`, `Promptfoo`, `LangSmith` 等在 Prompt 工程中的定位和价值。

---

### 4. 质量检查清单 (Quality Checklist)

- [ ] **核心类比一致性**：文章是否从头到尾都紧密围绕“Prompt as API”的类比展开？
- [ ] **工程可操作性**：提供的代码示例、文件命名规范和工具建议是否可以直接在真实项目中使用？
- [ ] **决策指导价值**：技术选型流程图是否清晰地帮助工程师在不同复杂度的任务面前做出合理选择？
- [ ] **生命周期完整性**：是否覆盖了从设计、开发、测试到治理的完整 API 生命周期？
- [ ] **前瞻性**：是否讨论了 Agent 等更复杂的 Prompt 模式，并将其自然地融入到工程体系中？
