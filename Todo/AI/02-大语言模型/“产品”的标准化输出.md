# AI Master Prompt: 为你的 LLM 编写“软件驱动程序”—— 一本关于生产级 LLM 应用稳健性的工程指南

**核心目标**：产出一篇面向 AI 工程师和系统架构师的权威指南，指导他们如何为大语言模型 (LLM) 构建一个生产级的“软件驱动程序”。本文旨在将与 LLM 的交互从不确定的“对话”转变为可靠的“系统调用”，确保应用的稳健性、安全性和可维护性。

**核心类比**：**将 LLM 视为一块功能强大但行为不羁的“硬件设备”。我们的任务是为其编写一个高质量的“驱动程序”，向上层应用提供稳定、可靠、标准化的接口，同时将底层硬件的复杂性和不确定性完全封装起来。**

---

### 1. 核心目标与原则 (The Core Goal & Principles)

*   **读者画像**：需要将 LLM 集成到真实业务系统，并对系统的可靠性、安全性和可维护性有极高要求的软件工程师、AI 工程师和技术架构师。
*   **风格**：极度务实，充满防御性编程和系统设计思维。提供清晰的模式、反模式、代码示例和决策流程图。
*   **“驱动程序”的核心职责**：
    1.  **协议定义 (Protocol Definition)**：定义与“硬件”通信的严格数据格式。
    2.  **指令执行 (Command Execution)**：安全地解析并执行“硬件”返回的指令。
    3.  **异常处理 (Exception Handling)**：优雅地处理所有预期的和非预期的“硬件”故障。
    4.  **持续集成 (Continuous Integration)**：通过自动化测试保证“驱动程序”的质量。

---

### 2. 文章结构 (Article Structure)

**引言：LLM - 一块需要“驱动”才能工作的强大“硬件”**
*   点明主题：直接使用 LLM API 就像直接操作硬件寄存器，充满风险。生产级应用需要一个健壮的“驱动程序”作为中间层。

**第一章：“驱动”的基石 —— 定义通信协议 (Structured Output)**

1.  **协议 V1：基于提示的 JSON (The "Gentleman's Agreement" Protocol)**
    *   类比：通过口头约定让硬件返回特定格式的数据。
    *   反模式：**必须明确指出这是一种反模式**。展示一个因模型幻觉导致 JSON 字符串解析失败的 Python `try-except` 示例，强调其脆弱性。
2.  **协议 V2：基于 JSON Schema 的强制约束 (The "Hardware Spec" Protocol)**
    *   类比：硬件出厂时自带规格说明书 (Spec)，保证输出符合基本语法。
    *   做法：重点介绍 OpenAI 的 `json_object` 模式。
    *   局限性：只能保证语法正确，无法保证内容合规。
3.  **协议 V3：基于文法的 Token 级约束 (The "Compiler-Level" Protocol)**
    *   类比：为硬件编写一个编译器，在生成每一个二进制位时都进行校验，确保输出 100% 符合预定义的任何复杂语法。
    *   **必须提供一个 `outlines` 或 `guidance` 库的简短代码示例**，展示如何用它来生成一个严格符合 Pydantic 模型的对象。这部分是可靠性的终极方案。

**第二章：“驱动”的核心 —— 安全的指令执行 (Safe Tool Use)**

1.  **核心设计原则：驱动程序是“执行者”，LLM 只是“建议者”**。
2.  **一个安全的“驱动程序”实现模式 (必须包含流程图和 Python 代码)**
    1.  **LLM 生成“指令建议”**：LLM 的输出是一个 `ToolCall` JSON 对象。
    2.  **驱动程序解析与验证**：**必须使用 Pydantic 模型**对 `ToolCall` 对象进行严格的解析和业务逻辑验证（权限、参数范围等）。
    3.  **沙箱化执行**：在隔离环境中（如 Docker）执行工具，并捕获所有 `stdout/stderr`。
    4.  **结果返回**：将执行结果封装后，作为下一次“驱动”调用的输入。

**第三章：“驱动”的韧性 —— 异常处理与容错机制 (Fault Tolerance)**

1.  **错误分类**：将 LLM 可能产生的错误类比为硬件错误：瞬时错误（如网络抖动）、固件 Bug（如特定输入下的胡言乱语）、逻辑错误（如错误的工具调用建议）。
2.  **容错模式 (Driver Resilience Patterns)**
    *   **重试 (Retry)**：处理瞬时错误。
    *   **带修复指令的重试 (Retry with Self-Correction)**：将错误信息和堆栈跟踪传回给 LLM，让它“自我修复”其输出。**必须提供一个具体的 Prompt 示例**。
    *   **优雅降级 (Graceful Degradation)**：多次失败后，驱动程序应返回一个默认值或错误码，而不是让整个应用崩溃。
3.  **幂等性：防止“指令”被重复执行**：强调所有工具的实现都必须是幂等的，这是构建可靠“驱动”的基石。

**第四章：“驱动”的质量保证 —— 自动化测试与 CI/CD (Quality Assurance)**

1.  **为“驱动”编写单元测试**：
    *   **黄金测试集 (Golden Set)**：为核心场景准备输入和期望的、结构化的输出。
    *   **断言 (Assertions)**：使用 `pytest` 编写测试用例，断言驱动程序的输出严格等于期望的 Pydantic 模型。
2.  **为“驱动”编写集成测试**：
    *   **AI 辅助评估**：使用更强的模型（如 GPT-4o）作为“裁判”，根据预设的评估维度 (Rubrics) 对驱动程序的复杂输出（如一段分析报告）进行打分。
    *   **CI/CD 集成**：**必须提及如何将 `promptfoo` 或 `LangSmith` 集成到 GitHub Actions 中**，在每次提交时自动运行评估，防止“驱动”性能衰退。

---

### 3. 工程深度与实践案例 (Engineering Depth & Practical Cases)

*   **决策流程图**：**必须包含一个 Markdown 文本描述的流程图**，指导开发者根据“可靠性要求”和“输出复杂度”来选择合适的“通信协议”（从简单 JSON 到 Grammar-based）。
*   **完整的“驱动程序”代码示例**：提供一个 `LLMDriver.py` 文件，其中包含一个 `LLMDriver` 类，封装了 `outlines`、`Pydantic` 验证、重试逻辑和工具调用，向上提供一个简洁的 `.execute()` 方法。

---

### 4. 质量检查清单 (Quality Checklist)

- [ ] **核心类比一致性**：文章是否从头到尾都紧密围绕“LLM 驱动程序”的类比展开？
- [ ] **代码可执行性**：提供的 `LLMDriver.py` 和测试代码是否清晰、完整，可以直接运行？
- [ ] **安全性与稳健性**：是否充分强调了验证、沙箱和幂等性等生产级要素？
- [ ] **可评估性**：CI/CD 集成方案是否具体，为 Prompt 的持续改进提供了可行的工程路径？
- [ ] **方案完整性**：是否清晰地对比了不同结构化输出方案的优劣，并给出了明确的选型指南？

