主要是三个核心阶段

1. **预训练**：构建世界观与知识体系
2. **指令微调**：学会与人对话
3. **人类反馈强化学习**：精炼言行，符合人类偏好

### 预训练

预训练是是最基础、最耗时、最耗费算力的阶段。使用来自互联网、书籍、学术论文等来源的万亿级 toke 组成的海量文本。

目标是让模型从海量无标注文本中学习“语言”和“世界”的统计规律。

核心任务是**自监督学习**。具体来说，是 **“下一个词预测”**。

通过预测以及和真实下一个词的比较，计算差距（损失），利用反向传播算法调整模型内部数以亿计的参数（神经网络的权重和偏置）。

然后，模型会微调这些参数，让它们朝着“下次猜得更准”的方向改变一点点。

这些步骤重复数十亿次甚至数万亿次。每一次，模型都在进行微小的进步。

**这个阶段的模型看作一个“饱读诗书但未经世事的隐士”**。他拥有庞大的知识库，能进行流畅的文本续写，但他还不懂得如何与人进行有问有答、有帮助、安全的对话。



### 指令微调

通过这个阶段，教会模型理解并执行人类的指令，如回答问题、写邮件、总结文章等。

需要人工精心构建的 **“指令-回答”** 对数据集。比如：

- 指令：“请用一句话解释什么是光合作用”
- 回答：“光合作用是植物利用光能将二氧化碳和水转化为有机物和氧气的过程”

我们使用这个高质量的指令数据集，在已经预训练好的模型上进行 **有监督微调**。

训练方式依然是预测下一个词，但此时的输入是“指令”，期望的输出是“高质量的回答”。

**经过这个阶段，我们的“隐士”开始步入社会，学会了基本的社交礼仪。** 他现在能够理解你的问题并给出相关的回答，而不是自顾自地续写。

但是，他的回答可能还不够精准、有用，或者有时会产生幻觉（编造事实），风格也可能不一致。



### 人类反馈强化学习

通过这个阶段，让模型的输出更加符合人类的主观偏好（更 helpful, honest, harmless）。

**RLHF 通常分为三个步骤：**

1. **收集人类反馈数据**：让人类标注员对同一个指令的多个模型输出进行排序
2. **训练一个“奖励模型”**：单独根据人类偏好训练一个模型来给回答打分
3. **使用强化学习优化模型**：让模型能获得奖励模型（裁判）的高分

**经过 RLHF 的洗礼，我们的“学者”终于成为了一位“良师益友”**。他不仅知识渊博，而且懂得如何清晰、有用、安全地与人交流。



| 阶段         | 目标                     | 数据                 | 比喻                               |
| :----------- | :----------------------- | :------------------- | :--------------------------------- |
| **预训练**   | 学习语言和世界的统计规律 | 海量无标注文本       | **“读万卷书”**，构建知识体系       |
| **指令微调** | 学会理解和执行人类指令   | 高质量的指令-回答对  | **“拜师学艺”**，学习对话技能       |
| **RLHF**     | 使输出符合人类主观偏好   | 人类对回答的排序数据 | **“社会实践”**，精炼言行，融入社会 |
