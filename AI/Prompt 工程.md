## 第一部分：Prompt 的设计与创作

一个优秀的 Prompt，其背后应该有清晰的设计逻辑，而非一时的灵光乍现。我们的目标是建立一套标准化的设计流程，让每一个 Prompt 都有据可依、有迹可循。

为了系统化地构建高质量 Prompt，我推荐使用 **CRISPE** 框架。它像一个清单，确保你在设计时不会遗漏关键要素。

- **C (Capacity & Role) - 能力与角色**
  - **本质**：为 AI 设定一个身份，激活其相关的知识库和思维模式。
  - **实践**：不要只说“帮我写代码”，而是说 `“你是一位拥有 10 年经验的前端架构师，精通 React 和性能优化。”`
  - **为什么重要**：明确的角色能让 AI 的回答更具专业性和深度，避免泛泛而谈。
- **R (Result) - 结果**
  - **本质**：清晰、无歧义地定义你期望的最终产出物。
  - **实践**：`“生成一份关于 React 状态管理库的选型报告。”` 这比 `“分析一下状态管理库”` 要好得多。
  - **为什么重要**：AI 的目标是满足你的需求。一个模糊的目标会导致一个模糊的答案。
- **I (Intent) - 意图**
  - **本质**：告诉 AI 你为什么要做这件事，提供上下文和背景。
  - **实践**：`“这份报告将用于团队技术分享，帮助初级开发者理解不同方案的优劣，并最终指导我们新项目的选型决策。”`
  - **为什么重要**：理解了意图，AI 才能更好地权衡信息的详略、侧重点和表达方式，产出更符合你真实场景的内容。
- **S (Style & Tone) - 风格与语气**
  - **本质**：规定输出的语言风格、格式和情感色彩。
  - **实践**：`“风格要求专业、严谨，使用 Markdown 格式，包含对比表格。语气要客观中立，多用数据和实例支撑观点。”`
  - **为什么重要**：确保输出内容能无缝集成到你的工作流中，无论是用于正式文档、内部邮件还是代码注释。
- **P (Persona) - 受众**
  - **本质**：设定 AI 回复的目标读者是谁。
  - **实践**：`“读者是团队的全栈开发者，他们对 React 有基础了解，但对复杂的状态管理原理不熟悉。”`
  - **为什么重要**：AI 会根据受众的知识水平调整术语使用和解释深度，让信息传递更高效。
- **E (Examples) - 示例**
  - **本质**：提供一到两个高质量的输入输出范例，即 Few-shot Learning。
  - **实践**：`“例如，对于 Redux，你可以这样分析：'优点：生态成熟，社区庞大。缺点：样板代码过多，学习曲线陡峭。适用场景：大型、复杂的应用状态管理。'”`
  - **为什么重要**：这是统一输出格式和质量的最有效手段。AI 会通过模仿你的示例，来“学会”你想要的回答模式。





## 第二部分：Prompt 的开发与集成

将 AI 的输出直接用于程序逻辑，最大的挑战是其不确定性。用正则表达式从一段自然语言中提取信息，既脆弱又痛苦。解决方案是强制 AI 输出结构化数据。

在 Prompt 的末尾，直接附上你期望的 JSON Schema。例如：

```json
{
  "name": "LibraryAnalysisReport",
  "type": "object",
  "properties": {
    "libraryName": { "type": "string" },
    "pros": { "type": "array", "items": { "type": "string" } },
    "cons": { "type": "array", "items": { "type": "string" } },
    "suitabilityScore": { "type": "number", "minimum": 0, "maximum": 10 }
  },
  "required": ["libraryName", "pros", "cons", "suitabilityScore"]
}
```

当 AI 需要调用外部工具（如查询数据库、调用 API）时，Function Calling 机制本身就是通过定义参数的 JSON Schema 来工作的。这与结构化输出的理念完全一致，确保了工具调用的可靠性。

即使有 Schema，AI 仍有可能在极少数情况下返回无效 JSON。因此，你的代码必须包含健壮的错误处理逻辑：

1. **解析失败重试**：如果 `JSON.parse` 抛出异常，可以尝试再次调用 AI，并在 Prompt 中提醒它“上次返回的不是有效 JSON，请严格遵守 Schema”。
2. **回退逻辑**：如果多次重试失败，应有降级方案，如记录错误、返回默认值或通知人工介入。





## 第三部分：Prompt 的评估与迭代

将 Prompt 视为模板，使用模板引擎（如 `Handlebars` for JavaScript, `Jinja` for Python）来管理。

创建一个 `generateReport.prompt` 文件：

```
你是一位{{role}}。
请生成一份关于{{topic}}的{{reportType}}。
这份报告的意图是{{intent}}。
风格要求：{{style}}。
读者是：{{persona}}。
请严格按照以下 JSON Schema 格式输出：
{{jsonSchema}}
```

实现了 Prompt 逻辑与数据的分离，提高了复用性，也使得动态生成 Prompt 变得简单。

我们可以将所有 `.prompt` 文件、相关的 Schema 文件以及评估脚本，全部纳入 Git 管理。

每次对 Prompt 的优化，都应该是一次有意义的 `commit`，并附上清晰的 `commit message`，如 `feat: optimize CRISPE intent for better code generation`。

每一次变更都可追溯、可比较、可回滚。这正是 `Prompt as Code` 的核心体现。



同时**拒绝体感，拥抱数据**，建立**评估维度**：

- `正确性 (Correctness)`：代码能否通过所有单元测试？(满分 5)
- `可读性 (Readability)`：代码是否符合团队规范，是否易于理解？(满分 3)
- `性能 (Performance)`：代码的时间和空间复杂度是否最优？(满分 2)

也可以邀请领域专家，根据上述 Rubrics 对 AI 的产出进行盲测打分。这是黄金标准，但成本高、速度慢。

用一个更强的模型（如 GPT-4）作为“裁判”。你给它一个评估 Prompt，包含原始任务、AI 的产出和详细的 Rubrics，让它来打分。

但是AI 裁判本身可能存在偏见，需要定期用人工评估的结果来校准它。





为你的核心任务，创建一个包含典型输入和期望输出（或评估标准）的数据集。

比如对于“代码生成”任务，你的数据集可能是一系列 `{ "description": "实现一个防抖函数", "test_cases": [...] }` 这样的对象。

- **自动化回归**
  - 将评估流程集成到 CI/CD 中。每次 git push 时，自动触发：
    1. 拉取最新的 Prompt 模板。
    2. 遍历黄金数据集中的每一个输入。
    3. 调用 LLM API 生成输出。
    4. 运行评估脚本（无论是单元测试还是 AI 裁判），计算各项指标的平均分。
    5. 生成评估报告，并与上一个版本的指标进行对比。
  - **优势**：确保你对 Prompt 的任何一次“优化”，都不是拆东墙补西墙。它保证了核心能力的稳定性，让迭代有据可依。





## 第四部分：Prompt 的治理

当 Prompt 数量增多时，需要一个统一的地方来管理它们，形成团队的宝贵资产。

可以是一个专门的 Git 仓库，也可以是集成在现有项目中的 `prompts/` 目录。关键是让团队成员能轻松地发现、理解和复用已有的 Prompt。

每个 Prompt 都应该有自带的文档。可以在 `.prompt` 文件头部使用 YAML Front Matter 来定义元数据：

```yaml
---
name: "Component Library Analysis"
version: "1.2.0"
author: "AI Team"
description: "用于生成前端组件库选型分析的 Prompt"
last_evaluated: "2023-10-27"
avg_score: 4.5
tags: ["frontend", "architecture", "analysis"]
---
```

定期组织 Prompt Review 会议，类似于 Code Review。分享优秀的 Prompt 设计，讨论遇到的难题，共同迭代 CRISPE 框架在团队内的应用范式。这能加速团队整体 Prompt 工程能力的提升。





### 总结

从遵循 **CRISPE** 框架进行精心设计，到利用 **JSON Schema** 实现稳定集成，再到建立 **评估体系** 和 **回归测试** 保证质量，最后通过 **资产库** 实现有效治理——我们构建了一个完整的闭环。

这套体系将 Prompt 从一次性的“指令”提升为可长期维护、持续增值的“资产”。它让 AI 的能力不再是团队中不可预测的“黑盒”，而是像任何一个优秀库或框架一样，可靠、高效、可控的强大工具。