**大语言模型本质上是一个经过海量数据训练的、极其复杂的“下一个词预测”机器。**

你可以理解为一个超级完形填空专家。

核心都是 **Transformer** 架构。它在长距离依赖的建模上显著优于 RNN/LSTM，但自注意力的计算复杂度随序列长度呈二次增长，处理超长文本仍然存在成本瓶颈。其最关键的两个机制是：

- **自注意力机制**：当读到一个词，能自动计算这个词与其他词的关联强度，动态地为每个词分配不同的“注意力权重”。
- **位置编码**：为每个词向量加入位置信息以保留顺序。常见实现包括绝对位置编码与旋转位置编码（RoPE）等。

大模型通常是 **自回归** 的。这意味着它一次只生成一个词（或 token），并将新生成的词作为输入的一部分，再去预测下一个词，如此循环。

当输入一个问题：

1. **输入处理：** 将你的问题（如“请解释一下引力”）转换成数字序列（tokenization）。
2. **层层计算：** 这个数字序列被送入由数十亿甚至上万亿参数组成的 Transformer 网络。数据会经过数十层甚至上百层的“前馈神经网络”和“自注意力层”进行计算，每一层都在提取和组合不同层次的特征。
3. **输出概率分布：** 经过所有层的计算后，模型会输出一个覆盖整个词汇表的概率分布。例如：
   - “是”： 15%
   - “的”： 10%
   - “原理”： 8%
   - “波”： 0.001%
   - ...（数十万个词）
4. **选择下一个词：** 模型会根据采样策略（如 Temperature、Top-k、Top-p）进行选择；在严谨任务中常使用低温或贪婪采样以提高确定性。采样的引入带来创造性和多样性。
5. **循环往复：** 将选中的词（比如“原理”）附加到原始输入后面，形成新的上下文（“请解释一下引力的原理”），然后重复步骤 1-4，直到生成完整的文本或达到停止条件。

我们有些控制参数：

| 参数            | 控制什么                 | 行为特点                       | 适用场景                                    |
| :-------------- | :----------------------- | :----------------------------- | :------------------------------------------ |
| **Temperature** | 概率分布的平滑度         | 低：保守、确定；高：随机、创意 | 需要平衡**连贯性**与**创造性**时            |
| **Top-k**       | 候选词的数量（固定）     | 保留概率最高的 k 个词          | 简单直接地提升质量，但不够灵活              |
| **Top-p**       | 候选词的累积概率（动态） | 自适应过滤，保留累积概率达到 p 的最小集合 | **通常首选**，与 Temperature 配合效果佳 |


| 任务类型                                      | 核心目标                 | Temperature   | Top-p          | 说明与效果                                                 |
| :-------------------------------------------- | :----------------------- | :------------ | :------------- | :--------------------------------------------------------- |
| **通用平衡（推荐起点）**                      | 兼顾连贯性与创造性       | **0.7**       | **0.9**        | 适用于大多数聊天、文案创作，是经过验证的“安全”选择。       |
| **高准确性任务** （代码生成、事实问答、翻译） | 精确、可靠、可预测       | **0.2 - 0.5** | **0.5 - 0.8**  | 低温使模型保守，专注于最高概率的选项，显著减少“胡言乱语”。 |
| **高创造性任务** （写诗、故事、头脑风暴）     | 多样、新颖、出人意料     | **0.8 - 1.2** | **0.95 - 1.0** | 高温赋予模型更多随机性，Top-p 放宽候选范围，激发创意。     |
| **开放域对话** （AI 伴侣、社交聊天）          | 自然、有趣、每次略有不同 | **0.7 - 0.9** | **0.9 - 0.95** | 在自然流畅的基础上，保持回复的多样性和新鲜感。             |
| **确定性输出（调试用）**                      | 追求极致稳定，用于测试   | **→ 0**       | **1**          | 接近“贪婪采样”，每次生成概率最高的词；可配合固定随机种子以保证复现性。 |

**优先搭配使用 `Temperature` 和 `Top-p`**

## 大模型的训练过程

主要是三个核心阶段

1. **预训练**：构建世界观与知识体系
2. **指令微调**：学会与人对话
3. **人类反馈强化学习**：精炼言行，符合人类偏好

### 预训练

预训练是最基础、最耗时、最耗费算力的阶段。使用来自互联网、书籍、学术论文等来源的、由万亿级 token 组成的海量文本。

目标是让模型从海量无标注文本中学习“语言”和“世界”的统计规律。

核心任务是**自监督学习**。具体来说，是 **“下一个词预测”**（next-token prediction）。

通过预测与真实下一个词的比较，计算差距（损失），利用反向传播算法调整模型内部数以亿计的参数（神经网络的权重和偏置）。

然后，模型会更新这些参数，让它们朝着“下次猜得更准”的方向改变一点点。

这些步骤重复数十亿次甚至数万亿次。每一次，模型都在进行微小的进步。

**这个阶段的模型可看作一个“饱读诗书但未经世事的隐士”**。他拥有庞大的知识库，能进行流畅的文本续写，但他还不懂得如何与人进行有问有答、有帮助、安全的对话。


### 指令微调

通过这个阶段，教会模型理解并执行人类的指令，如回答问题、写邮件、总结文章等。

需要人工精心构建的 **“指令-回答”** 对数据集。例如：

- 指令：“请用一句话解释什么是光合作用”
- 回答：“光合作用是植物利用光能将二氧化碳和水转化为有机物和氧气的过程”

我们使用这个高质量的指令数据集，在已经预训练好的模型上进行 **有监督微调**。

训练方式依然是预测下一个词，但此时的输入是“指令”，期望的输出是“高质量的回答”。

**经过这个阶段，我们的“隐士”开始步入社会，学会了基本的社交礼仪。** 他现在能够理解你的问题并给出相关的回答，而不是自顾自地续写。

但是，他的回答可能还不够精准、有用，或者有时会产生幻觉（编造事实），风格也可能不一致。


### 人类反馈强化学习

通过这个阶段，让模型的输出更加符合人类的主观偏好（更 helpful、honest、harmless）。

**RLHF 通常分为三个步骤：**

1. **收集人类反馈数据**：让人类标注员对同一个指令的多个模型输出进行排序
2. **训练一个“奖励模型”**：单独根据人类偏好训练一个模型来给回答打分
3. **使用强化学习优化模型**：使用如 PPO 等策略优化方法，让模型能获得奖励模型（裁判）的高分

**经过 RLHF 的洗礼，我们的“学者”终于成为了一位“良师益友”**。他不仅知识渊博，而且懂得如何清晰、有用、安全地与人交流。


| 阶段       | 目标                     | 数据                 | 比喻                     |
| :--------- | :----------------------- | :------------------- | :----------------------- |
| **预训练** | 学习语言和世界的统计规律 | 海量无标注文本       | **“读万卷书”**，构建知识体系 |
| **指令微调** | 学会理解和执行人类指令   | 高质量的指令-回答对  | **“拜师学艺”**，学习对话技能   |
| **RLHF**   | 使输出符合人类主观偏好   | 人类对回答的排序数据 | **“社会实践”**，精炼言行，融入社会 |
