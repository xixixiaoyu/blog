## 引言：从“感觉良好”到“数据驱动”

传统软件的质量，可以用 `assert(add(2, 2) === 4)` 来精确衡量。而一个 AI 应用，特别是基于 LLM 的应用，其质量好坏往往充满了**主观性**和**非确定性**。我们无法再用简单的断言来衡量“好”。

如果我们不能科学地度量“好”，那么一切优化都将是凭“感觉”行事，这在工程上是不可接受的。

因此，我们必须建立一套系统性的**数据与评估体系**。它就像一个持续旋转的**迭代飞船**，载着我们的 AI 产品，从充满不确定性的“创意空间”驶向创造真实业务价值的“商业大陆”。这个飞船由四个核心引擎驱动：

1.  **数据准备 (Fuel)**：为飞船注入高质量的燃料。
2.  **离线评估 (Simulator)**：在模拟环境中测试飞船的性能。
3.  **在线评估 (Real-World Test)**：在真实世界中检验飞船的价值。
4.  **数据闭环 (Navigation System)**：根据真实世界的反馈，自动修正航向。

## 第一部分：评估金字塔：从单元到业务的立体化度量

单一的评估方法是片面的。我们需要一个从微观到宏观的立体评估体系，我们称之为“评估金字塔”。

<img src="https://example.com/pyramid.png" alt="评估金字塔" width="600" />

#### **第一层：单元评估 (Unit Evaluation)**

*   **目标**：在“实验室”中，快速、低成本地验证模型或 Prompt 的单点能力。
*   **核心**：使用精心构建的“黄金评估集”。
*   **方法**：
    1.  **有标准答案的评估**：对于分类、提取等任务，使用**准确率、精确率、召回率、F1-Score** 等传统指标。
    2.  **无标准答案的评估**：这是 LLM 评估的核心。我们主要依赖 **“AI 裁判” (AI as a Judge)**。

**深度实践：“AI 裁判”的设计**

“AI 裁判”的核心是设计一个高质量的评估提示 (Rubrics)。它必须包含五个要素：

1.  **角色 (Role)**：`“你是一位拥有 10 年经验的资深软件架构师。”`
2.  **任务 (Task)**：`“请对以下两个由不同模型生成的代码实现进行比较。”`
3.  **维度 (Dimensions)**：`“评估维度包括：正确性（能否运行）、可读性（是否清晰）、性能（是否高效）、安全性（有无漏洞）。”`
4.  **标准 (Criteria)**：为每个维度提供清晰的打分标准。`“正确性：能通过所有测试用例得 5 分，部分通过得 3 分，无法运行得 1 分。”`
5.  **输出格式 (Output Format)**：要求返回结构化的 JSON，便于程序解析和统计。

```json
// AI 裁判的输出示例
{
  "model_a_score": {
    "correctness": 5,
    "readability": 3,
    "performance": 4,
    "security": 5
  },
  "model_b_score": { ... },
  "winner": "model_a",
  "justification": "模型 A 的代码虽然可读性稍差，但通过了所有测试用例，且在性能上明显优于模型 B..."
}
```

#### **第二层：端到端评估 (End-to-End Evaluation)**

*   **目标**：在模拟环境中，评估 Agent 完成一个完整业务流程的能力。
*   **核心**：构建一套覆盖核心业务场景的“红线”用例集。
*   **方法**：
    *   **Agent Trajectory 评估**：对于需要多步工具调用的 Agent，我们不仅要看最终结果，还要评估其每一步的“思考-行动”轨迹是否合理。
    *   **AI 裁判** 在这里依然适用，但评估的维度会更宏观，如 `任务完成率`、`工具调用准确率`、`成本`、`延迟` 等。

#### **第三层：在线评估 (Online Evaluation)**

*   **目标**：在真实世界中，衡量 AI 应用对**核心业务指标**的最终影响。
*   **核心**：将技术指标与商业价值挂钩。
*   **方法**：
    1.  **A/B 测试**：这是黄金标准。将用户随机分流，比较新旧两个版本的**用户转化率、留存率、任务完成时长**等业务指标。
    2.  **影子模式 (Shadow Mode)**：在不影响用户的前提下，将线上流量同时发给新旧模型，在后端比较其输出差异、延迟和成本。这是一种低风险的“静默测试”。
    3.  **人工评估**：建立内部平台，让真人对线上抽样的模型输出进行精细化标注（如 `准确` / `不准确` / `有害`），为数据闭环提供高质量的“养料”。

## 第二部分：数据闭环：AI 应用的自我进化引擎

评估的最终目的，是发现问题，并驱动产品自我进化。一个高效的数据闭环，是实现这一目标的关键。

#### **数据闭环工作流**

<img src="https://example.com/workflow.png" alt="数据闭环工作流" width="700" />

1.  **数据采集 (Collect)**：通过前端埋点（显式反馈：点赞/踩；隐式反馈：复制、修改）和后端日志（完整记录 Prompt、Response、Latency、Cost 等），捕捉一切有价值的信号。
2.  **问题发现 (Identify)**：通过在线评估（如 A/B 测试指标下降）或人工评估（如大量“不准确”标签），定位到有问题的场景 (Bad Cases)。
3.  **数据沉淀 (Enrich & Store)**：这是闭环的核心。将发现的 Bad Cases 转化为两种宝贵的资产：
    *   **加入黄金集**：将典型的失败案例及其理想答案，加入到端到端回归测试的“黄金集”中，确保问题不再复现。
    *   **生成微调数据**：将修正后的问答对，转化为符合特定格式（如 JSONL）的微调数据。
4.  **触发优化 (Trigger)**：当积累了足够的高质量微调数据后，自动触发 CI/CD 流水线，启动新一轮的模型微调、离线评估和在线部署。

#### **工程实现伪代码**

```javascript
// 简化的数据闭环工作流
async function processUserFeedback(feedback) {
  // 1. 采集：收到用户负反馈
  if (feedback.type === 'negative') {
    // 2. 发现：从日志中拉取完整交互记录
    const interactionLog = await getInteractionLog(feedback.sessionId)
    
    // 3. 沉淀：创建人工审核任务
    const reviewTask = createReviewTask({
      prompt: interactionLog.prompt,
      response: interactionLog.response,
      feedback: feedback.text
    })
    
    // 人工审核后，得到修正后的理想答案
    const idealAnswer = await waitForHumanReview(reviewTask)
    
    // 4. 触发：将修正后的数据分别加入黄金集和微调集
    goldenSet.add({ 
      prompt: interactionLog.prompt, 
      expected_answer: idealAnswer 
    })
    
    finetuningDataset.add({ 
      instruction: interactionLog.prompt, 
      output: idealAnswer 
    })
    
    // 当微调数据达到阈值，自动触发训练
    if (finetuningDataset.size() >= TRAINING_THRESHOLD) {
      triggerTrainingPipeline()
    }
  }
}
```

在整个过程中，**数据合规与隐私**是不可逾越的红线。必须严格遵守 GDPR 等法规，对所有个人可识别信息 (PII) 进行匿名化处理。

## 结论：评估体系是 AI 应用的“战略罗盘”

科学的评估体系，为我们解决了 AI 应用开发的三个核心问题：

1.  **“好”的定义**：它将模糊的主观感受，量化为可追踪、可比较的数据指标。
2.  **进化的方向**：它通过数据闭环，告诉我们应该优化什么、如何优化，为产品迭代指明了方向。
3.  **价值的证明**：它通过在线 A/B 测试，将技术优化与商业成功联系起来，最终证明了 AI 应用的业务价值。

构建 AI 应用，不是在寻找一个完美的“终极模型”，而是在打造一个能够持续学习和进化的“生命体”。而这套**数据与评估体系**，正是这个生命体赖以生存和成长的“新陈代谢系统”。它定义了产品的“品味”，决定了进化的方向，是连接技术与商业价值的核心桥梁。
