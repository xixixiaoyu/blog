预训练模型的知识与风格在训练完成后基本固定。

如果我们想让模型学习新的领域知识、任务能力或风格，微调是一个很好的方案。


#### 全参数微调 - “全员再培训”

让模型在特定数据上重新学习，并**更新所有的权重参数**。

效果通常最好，但需要巨大的计算资源和存储空间（不仅是原始模型，还包括梯度与优化器状态）。


#### 参数高效性微调（PEFT） - “聘请专家顾问团”

这是目前的主流与重要发展方向。

核心思想是：**冻结预训练模型的大部分参数，只训练一小部分新增的适配器参数**。


常见的 PEFT 技术包括：

- **LoRA**：假设模型参数的变化是低秩的。我们不做全参数更新，而是用两个小得多的矩阵（A 和 B）的乘积来近似更新量（ΔW ≈ B·A，秩 r 远小于原维度）。效果好、训练量小、速度快、存储成本低（只需保存 A 和 B）。
- **QLoRA**：在 LoRA 的基础上进一步降低显存，**将预训练模型权重量化为 4-bit 进行存储与加载**，训练时对量化权重进行近似反量化参与计算（通常使用 FP16/BF16 进行前向/反向与适配器训练）。这显著降低显存占用，使得在消费级显卡（如 24GB 显存）上微调 7B–13B 乃至部分 33B 模型成为可能；更大的 65B/70B 模型通常需要更大显存或多卡并行。
- **P-Tuning / Prefix-Tuning**：在模型的输入前添加一段可训练的“软提示”（连续向量），或在每层注意力中注入可训练前缀键值，通过优化这段提示来引导模型产生期望的输出。


#### 实践步骤

- 明确目标：希望模型具体学会什么（领域知识、任务格式、风格、行为约束等）。
- 准备高质量数据（通常几百到几千条即可）：常组织成 `{"instruction": "...", "input": "...", "output": "..."}` 的对话/任务样式。
- 选择方案：全参数微调或 LoRA/QLoRA 等高效方法。
- 训练与评估：
  - 在训练集上训练模型。
  - 在**未见过的验证集**上评估效果，防止过拟合。
  - 指标可包含困惑度（PPL），但更重要的是**人工评测**与任务型指标（如准确率、BLEU/ROUGE、代码可运行性等）。
- **部署应用**：将微调好的模型部署到生产环境（服务化/推理加速/监控与回滚策略）。