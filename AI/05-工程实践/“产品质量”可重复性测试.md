# Master Prompt: 将 RAG 评估类比为“编写单元测试和集成测试”

## 1. 核心目标 (Core Goal)

你是一位资深的测试开发工程师 (SDET)，你的任务是为一群经验丰富的软件工程师撰写一份关于“如何为 RAG 应用编写自动化测试”的实战指南。这份指南需要将看似复杂的 LLM 评估问题，完全转化为他们所熟知的单元测试和集成测试的工程实践。

**核心类比**：
- **RAG 应用评估**: 为一个复杂的、包含“数据库查询”（检索）和“业务逻辑”（生成）的系统 **编写测试**。
- **评估框架 `ragas`**: 一个专门为 RAG 应用设计的“**测试框架**”，类似于 `Pytest` 或 `Jest`。
- **评估数据集 (Golden Dataset)**: 测试代码中的“**Mock 数据和期望结果**”。它包含了一系列的问题 (`question`)、参考答案 (`ground_truth`) 以及用于回答问题的上下文 (`ground_truth_context`)。
- **评估指标 (Metrics)**: 测试框架中的“**断言函数 (Assertions)**”。例如，`assert faithfulness` 就是一个断言，用来检查模型的回答是否“忠于”给定的上下文。
- **`ragas.evaluate()`**: 运行测试的命令，相当于 `pytest` 或 `npm test`。
- **评估报告**: 最终生成的“**测试覆盖率和结果报告**”，清晰地展示了系统的哪些部分表现良好，哪些部分存在问题。

本文的目标是提供一份纯粹的测试工程视角指南，让开发者能够将他们现有的测试技能和思维模式，无缝地应用到保障 RAG 应用质量的工作中。

## 2. 文章结构 (Article Structure)

**I. 标题：RAGAS-FIRST：像写测试一样评估你的 RAG 应用**

**II. 引言：当 `assert result == expected` 失效时，我们该如何测试？**
   - 破题：LLM 的非确定性让我们无法使用传统的确定性断言，但测试的“分而治之”思想依然有效。
   - 核心思想：我们将 RAG 系统拆分为 **检索 (Retriever)** 和 **生成 (Generator)** 两个核心组件，并分别为它们编写“单元测试”，最后再进行“集成测试”。

**III. 准备“测试数据”：构建你的评估数据集**
   - **什么是好的评估数据集**: 包含多样化的问题、高质量的参考答案和对应的上下文来源。
   - **Code Snippet**: 展示如何使用 `datasets` 库创建一个包含 `question`, `answer`, `contexts`, `ground_truth` 四个字段的“黄金数据集”。

**IV. 为 RAG 编写“单元测试”**
   - **A. 测试检索器 (Retriever): 它找对了吗？找全了吗？**
     - **核心指标**: 
       - `Context Precision`: 检索到的上下文中，有多少是真正相关的？（查准率）
       - `Context Recall`: 所有相关的上下文中，有多少被成功检索出来了？（查全率）
     - **解读**: 这两个指标帮助我们判断是“知识库”本身有问题，还是“检索算法”有问题。

   - **B. 测试生成器 (Generator): 它“忠诚”吗？它“切题”吗？**
     - **核心指标**: 
       - `Faithfulness` (忠实度): 模型的回答中有多少信息是可以在给定的上下文中找到依据的？（防止幻觉）
       - `Answer Relevancy` (相关性): 模型的回答在多大程度上回应了用户的原始问题？（防止答非所问）
     - **解读**: 这两个指标帮助我们判断 Prompt 设计得好不好，模型本身的能力是否足够。

**V. 为 RAG 编写“集成测试”**
   - **核心指标**: `Answer Correctness` (答案正确性)
     - **工作原理**: `ragas` 会综合评估生成的答案和参考答案 (`ground_truth`) 之间的语义相似度。
     - **解读**: 这是对整个 RAG 管道端到端表现的最终衡量，是面向用户的“验收测试”。

**VI. 实战：用 `ragas` 运行你的第一个 RAG 测试套件**
   - **Code Snippet**: 提供一个完整的、可执行的 Python 脚本，包含以下步骤：
     1.  **准备数据**: 使用 `datasets.Dataset.from_dict` 创建一个包含几条样本的评估数据集。
     2.  **模拟 RAG 输出**: 模拟一个 RAG 应用的输出结果，包含 `answer` 和 `contexts` 字段。
     3.  **选择断言**: 从 `ragas.metrics` 中导入 `context_precision`, `faithfulness`, `answer_correctness` 等“断言函数”。
     4.  **运行测试**: 调用 `ragas.evaluate()` 并传入数据集和指标。
     5.  **解读报告**: 打印出返回的 `Result` 对象，它就像一份 Pytest 的测试报告。

   ```python
   from datasets import Dataset
   from ragas import evaluate
   from ragas.metrics import (
       faithfulness,
       answer_relevancy,
       context_recall,
       context_precision,
       answer_correctness,
   )

   # 1. 准备“Mock 数据和期望结果”
   golden_dataset_dict = {
       "question": [
           "What is the capital of France?",
           "Who wrote 'To Kill a Mockingbird'?"
       ],
       "answer": [
           "The capital of France is Paris.",
           "'To Kill a Mockingbird' was written by Harper Lee."
       ],
       "contexts": [
           ["France is a country in Western Europe. Its capital is Paris."],
           ["Harper Lee is an American novelist, famous for her book 'To Kill a Mockingbird'."]
       ],
       "ground_truth": [
           "The capital of France is Paris, a major European city and a global center for art, fashion, and culture.",
           "Harper Lee's novel 'To Kill a Mockingbird' was published in 1960 and won the Pulitzer Prize."
       ]
   }
   golden_dataset = Dataset.from_dict(golden_dataset_dict)

   # 2. 运行评估，相当于 `pytest`
   # ragas 会自动处理对 LLM 的调用以进行评估
   result = evaluate(
       dataset=golden_dataset,
       metrics=[
           context_precision,  # Retriever 单元测试
           context_recall,     # Retriever 单元测试
           faithfulness,       # Generator 单元测试
           answer_relevancy,   # Generator 单元测试
           answer_correctness, # 端到端集成测试
       ],
       # llm=your_evaluation_llm, # 可以指定用于评估的 LLM
       # embeddings=your_embedding_model, # 可以指定用于计算相似度的 embedding
   )

   # 3. 查看“测试报告”
   print(result)
   # 你会得到一个类似测试报告的字典，包含每个指标的分数
   # {'context_precision': 1.0, 'context_recall': 1.0, 'faithfulness': 1.0, ...}
   ```

**VII. 结论：测试驱动的 LLM 开发 (TDD for LLM)**
   - 总结：通过 `ragas`，我们可以将成熟的测试工程思想引入到 LLM 应用开发中，实现质量的量化、持续和自动化。
   - 展望：将 `evaluate` 集成到 CI/CD 流水线中，建立“质量门禁”，实现真正的“测试驱动的 LLM 开发”。

## 3. 质量与风格核对清单 (Quality & Style Checklist)

- [ ] **测试术语**: 是否通篇使用“单元测试”、“集成测试”、“断言”、“Mock 数据”、“测试报告”等软件测试工程师熟悉的术语？
- [ ] **类比一致性**: `ragas` 与 `Pytest`/`Jest` 的类比是否清晰并贯穿全文？
- [ ] **代码可执行**: 提供的 Python 代码片段是否是一个完整的、可直接运行的示例？
- [ ] **问题导向**: 是否清晰地解释了如何通过不同的指标来定位 RAG 系统中（检索器 vs. 生成器）的问题？
- [ ] **概念清晰**: 是否清晰地区分了 `Context Precision/Recall`（测试检索器）、`Faithfulness/Answer Relevancy`（测试生成器）和 `Answer Correctness`（端到端测试）之间的不同作用？
