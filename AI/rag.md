大语言模型是基于海量文本训练的统计语言模型，训练完成后，其参数中“压缩”的知识基本固定（不继续训练就不会随时间更新）。

RAG（Retrieval-Augmented Generation，检索增强生成）将**信息检索（Retrieval）**与**文本生成（Generation）**解耦并重新组合：先检索相关资料，再让模型基于资料回答，从而缓解幻觉并弥补静态知识的不足。

简单理解就是让大模型“查资料再作答”。


### 步骤

1. 索引
   1. **加载与分块**：将长文档切分成更小、语义完整的“文本块”（合理的 chunk 大小与重叠能提升召回）
   2. **向量化**：使用嵌入模型将文本块转换为**向量**（embedding）
   3. **存储**：将向量及其对应的原文与元数据（来源、时间戳、标签等）存入**向量数据库**

2. 检索
   1. **查询向量化**：将用户的查询语句使用**同一个嵌入模型**转换为查询向量
   2. **相似性搜索**：在向量数据库中进行**相似度计算**，召回与查询向量最接近的 Top-k 文本块
   3. **重排（可选）**：用交叉编码器或规则对召回结果进行**重新排序**，提升语义相关性与可读性
   4. **返回上下文**：提取前若干条最相关的原始文本块作为“上下文”

3. 生成
   1. 让大模型基于检索到的上下文和用户的问题，生成高质量答案。常见提示模板示例：
      
      ```
      请严格根据以下提供的信息来回答问题；如果信息不足以回答，请明确说明你不知道，并不要编造。
      
      【相关信息开始】
      {检索到的上下文}
      【相关信息结束】
      
      问题：{用户的问题}
      答案（如可能请给出来源或引用片段）：
      ```


### 常见注意事项

- 嵌入模型应与语料/查询风格匹配；必要时可做领域适配或中文专用模型
- 合理的 chunk 设计很重要：常用 200–800 tokens，适当 10%–30% 重叠以保留跨段语义
- 结合**过滤条件**（元数据）与**重排**可显著提升相关性与可读性
- 当上下文窗口有限时，可对召回内容进行**压缩/摘要**，或采用多轮检索（递归检索）
- 评估可关注“是否基于提供证据”（groundedness）、准确性与可引用性


RAG 的适用场景：**客服机器人**、**知识库问答**、**AI 辅助研究**、**内容创作与事实核查**等。