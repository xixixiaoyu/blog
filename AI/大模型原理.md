**大语言模型本质上是一个经过海量数据训练的、极其复杂的“下一个词预测”机器。**

你可以理解为一个超级完形填空专家。

核心都是 **Transformer** 架构。它解决了传统模型在处理长文本时的瓶颈。其最关键的两个机制是：

- **自注意力机制**：当读到一个词，能自动计算这个词与其他词的关联强度，它动态地为每个词分配不同的“注意力权重”
- **位置编码**：输入时，会为每个词向量加上一个表示其位置信息的编码

大模型是 **自回归** 的。这意味着它一次只生成一个词（或 token），并将新生成的词作为输入的一部分，再去预测下一个词，如此循环。

当输入一个问题：

1. **输入处理：** 将你的问题（如“请解释一下引力”）转换成数字序列（tokenization）
2. **层层计算：** 这个数字序列被送入由数十亿甚至上万亿参数组成的 Transformer 网络。数据会经过数十层甚至上百层的“前馈神经网络”和“自注意力层”进行计算。每一层都在提取和组合不同层次的特征
3. 输出概率分布：经过所有层的计算后，模型会输出一个覆盖整个词汇表的概率分布。例如：
   - “是”： 15%
   - “的”： 10%
   - “原理”： 8%
   - “波”： 0.001%
   - ...（数十万个词）
4. **选择下一个词：** 模型并不只是选择概率最高的词，那样非常枯燥无聊，它会通过采样策略从高概率的候选词中随机选择一个。这引入了创造性和随机性
5. **循环往复：** 将选中的词（比如“原理”）附加到原始输入后面，形成新的上下文（“请解释一下引力 原理”），然后重复步骤 1-4，直到生成一个完整的句子或达到停止条件

我们有些控制参数：

| 参数            | 控制什么                 | 行为特点                       | 适用场景                                    |
| :-------------- | :----------------------- | :----------------------------- | :------------------------------------------ |
| **Temperature** | 概率分布的平滑度         | 低：保守，确定；高：随机，创意 | 需要平衡**连贯性**与**创造性**时            |
| **Top-k**       | 候选词的数量（固定）     | 过滤绝对低概率词               | 简单直接地提升质量，但不够灵活              |
| **Top-p**       | 候选词的累积概率（动态） | 自适应过滤，根据分布形状调整   | **通常首选**，与 Temperature 配合使用效果佳 |