# 写作提示 —— 模型服务与部署（vLLM、TGI、Ollama）

使用说明：
- 面向后端/平台工程与前端协作，生成“如何部署与运维推理服务”的落地指南。
- 聚焦主流推理服务：vLLM、TGI（Text Generation Inference）、Ollama，以及私有化/本地化部署。

生成目标：
- 比较不同推理服务的能力、资源需求、性能与生态（API 兼容、扩展性）。
- 给出部署与配置步骤（容器、GPU/CPU、显存规划、量化策略、并发参数）。
- 总结扩缩容、负载均衡、滚动升级、A/B 测试与回滚策略。
- 提供多云/多供应商的路由与降级方案，保证稳定性与成本可控。

大纲建议：
1. 选型维度（模型支持、性能、易用性、生态与社区）
2. 环境与资源规划（GPU/CPU、显存与内存、网络带宽）
3. 部署与配置（容器镜像、参数、量化、并发与队列）
4. 路由与扩缩容（负载均衡、自动化伸缩、优先级调度）
5. 升级与回滚（版本管理、兼容策略、灰度发布）
6. 观测与运维（日志、指标、Tracing、告警）
7. 安全与合规（访问控制、隔离、清理与审计）

输出格式要求：
- Markdown；附示例部署命令/配置片段（Docker/K8s）与参数解释。
- 给出性能/成本对比的文字表或清单（QPS、延迟、显存占用）。

质量检查清单：
- 步骤可执行；参数含义明确，能复现性能指标。
- 有升级与回滚策略；不影响线上服务稳定性。
- 兼顾安全与合规；权限与访问控制清晰。

