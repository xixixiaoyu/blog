# 写作提示 —— 成本与缓存

使用说明：
- 生成控制 LLM 成本与提升响应效率的工程实践文章。

生成目标：
- 解释 Token 成本、上下文长度与调用次数之间的关系。
- 总结缓存策略（请求级缓存、提示级缓存、结果重用、Embedding 缓存）。
- 给出成本监控与优化清单（参数、复用、裁剪、分层检索）。

大纲建议：
1. 成本模型（按调用/按 Token）与参数影响（温度、Top-k/p）
2. 缓存策略分类（请求、提示、结果、向量）
3. 复用与裁剪（上下文压缩、摘要、引用）
4. 分层检索与候选集（粗检/精排）
5. 观测与告警（成本仪表、预算阈值、异常模式）
6. 与用户体验的权衡（延迟、质量、成本）

输出格式要求：
- Markdown；提供最小缓存中间件示例与参数建议。
- 给出成本度量的日志字段与报表建议。

质量检查清单：
- 方案可实操；缓存策略与一致性处理明确。
- 有观测与阈值控制；能防止成本失控。
- 结合真实场景提出权衡与折衷。

默认技术栈：TypeScript + NestJS

最小缓存中间件示例（NestJS CacheModule + CacheInterceptor）

依赖：`npm i cache-manager @nestjs/cache-manager`

```ts
// src/app.module.ts
import { Module, CacheModule } from '@nestjs/common';

@Module({
  imports: [
    CacheModule.register({ ttl: 60, max: 1000 }), // 60s 默认 TTL，最多 1000 条
  ],
})
export class AppModule {}
```

```ts
// src/cache.interceptor.ts
import { Injectable, CacheInterceptor, ExecutionContext } from '@nestjs/common';
import { createHash } from 'crypto';

@Injectable()
export class LlmCacheInterceptor extends CacheInterceptor {
  trackBy(context: ExecutionContext): string | undefined {
    const req = context.switchToHttp().getRequest();
    if (!req) return undefined;
    // 基于路由 + Body 生成缓存键（提示级缓存）
    const keyRaw = `${req.method}:${req.originalUrl}:${JSON.stringify(req.body)}`;
    return createHash('sha256').update(keyRaw).digest('hex');
  }
}
```

```ts
// src/llm.controller.ts
import { Controller, Post, Body, UseInterceptors } from '@nestjs/common';
import { LlmCacheInterceptor } from './cache.interceptor';

@Controller('llm')
export class LlmController {
  @Post('chat')
  @UseInterceptors(LlmCacheInterceptor)
  async chat(@Body() body: { prompt: string }) {
    // 调用上游模型（省略）
    return { text: '...' };
  }
}
```

结果重用与多层缓存建议：
- 请求级：同一提示在短期内相同结果；
- 提示级：归一化提示（去时间戳、用户标识）以提高命中；
- 向量级：Embedding 结果缓存（文本 → 向量），减少重复计算；
- 分层：内存 → Redis；按租户与场景划分缓存域。

成本度量与日志字段建议：
- `provider`、`model`、`inputTokens`、`outputTokens`、`pricePerMTok`、`costUsd`、`latencyMs`、`cached`（命中与否）
- 当 SDK 提供 `usage` 字段时直接记录；否则可用近似估计（如基于 BPE 近似切分）。

优化清单（文字版）：
- 参数：降低温度与 Top-p/k；必要时缩短上下文或采用摘要/引用；
- 复用：复用系统提示与公共上下文；多轮对话采用窗口截断；
- 检索：分层检索（粗检/精排）减少候选上下文规模；
- 缓存：embedding 与提示级缓存；对不可缓存请求使用短 TTL；
